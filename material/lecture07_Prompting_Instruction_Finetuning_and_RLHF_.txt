Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Prompting Instruction Finetuning and RLHFReminders Project proposals will be posted this week. A2 due Saturday 11 59PM We still recommend using Colab for the assignments in case you run into trouble e.g. you have exceeded Colab quota Please contact us 2Larger and larger models 3 https www.economist.com interactive briefing 2022 06 11 huge foundation models are turbo charging ai progress GPT 4 has 1.7 trillion parametersTrained on more and more data tokens seen during training https babylm.github.io 4 GPT 4 has seen 13 trillion tokensRecap What kinds of things does pretraining learn 5 Stanford University is located in California. Trivia I put fork down on the table. syntax The woman walked across the street checking for traffic over shoulder. coreference I went to the ocean to see the fish turtles seals and . lexical semantics topic Overall the value I got from the two hours watching it was the sum total of the popcorn and the drink. The movie was . sentiment Iroh went into the kitchen to make some tea. Standing next to Iroh Zuko pondered his destiny. Zuko left the . some reasoning this is harder I was thinking about the sequence that goes 1 1 2 3 5 8 13 21 some basic arithmetic they don t learn the Fibonnaci sequenceLanguage models as world models Language Models as Agent Models Andreas 2022 6 Language models may do rudimentary modeling of agents beliefs and actionsLanguage models as world models https www.khanacademy.org test prep sat x0a8c2e5f untitled 652 7 mathLanguage models as world models https github.com features copilot 8 codeLanguage models as world models Larnerd 2023 medicine 9Language models as multitask assistants Microsoft Bing Also see OpenAI s ChatGPT Google s Bard Anthropic s Claude 10Language models as multitask assistants How do we get from this McMaster University is located in to this 11Lecture Plan From Language Models to Assistants 12 1. Zero Shot ZS and Few Shot FS In Context Learning 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLecture Plan From Language Models to Assistants 13 1. Zero Shot ZS and Few Shot FS In Context Learning 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLet s revisit the Generative Pretrained Transformer GPT models from OpenAI as an example Emergent abilities of large language models GPT 2018 GPT 117M parameters Radford et al. 2018 Transformer decoder with 12 layers. Trained on BooksCorpus over 7000 unique books 4.6GB text . Showed that language modeling at scale can be an effective pretraining technique for downstream tasks like natural language inference. entailment START The man is in the doorway DELIM The person is near the door EXTRACT 14 DecoderEmergent abilities of large language models GPT 2 2019 Let s revisit the Generative Pretrained Transformer GPT models from OpenAI as an example GPT 2 1.5B parameters Radford et al. 2019 Same architecture as GPT just bigger 117M 1.5B But trained on much more data 4GB 40GB of internet text data WebText Scrape links posted on Reddit w at least 3 upvotes rough proxy of human quality GPT 2018 GPT 2 2019 117M 1.5B 15One key emergent ability in GPT 2 is zero shot learning the ability to do many tasks with no examples and no gradient updates by simply Specifying the right sequence prediction problem e.g. question answering Passage Tom Brady... Q Where was Tom Brady born A ... Comparing probabilities of sequences e.g. Winograd Schema Challenge Levesque 2011 16 The cat couldn t fit into the hat because it was too big. Does it the cat or the hat Is P ...because the cat was too big P ...because the hat was too big Emergent zero shot learning Radford et al. 2019Emergent zero shot learning GPT 2 beats SoTA on language modeling benchmarks with no task specific fine tuning LAMBADA language modeling w long discourse dependencies Paperno et al. 2016 Radford et al. 2019 17Emergent zero shot learning 18 You can get interesting zero shot behavior if you re creative enough with how you specify your task Summarization on CNN DailyMail dataset See et al. 2017 SAN FRANCISCO California CNN A magnitude 4.2 earthquake shook the San Francisco ... overturn unstable Radford et al. 2019 2018 SoTA Supervised 287K Too Long Didn t Read Prompting objects. TL DR Select from article ROUGEEmergent abilities of large language models GPT 3 2020 19 GPT 3 175B parameters Brown et al. 2020 Another increase in size 1.5B 175B and data 40GB over 600GB 117M 1.5B GPT 2018 GPT 2 2019 GPT 3 2020 175BEmergent few shot learning Specify a task by simply prepending examples of the task before your example Also called in context learning to stress that no gradient updates are performed when learning a new task there is a separate literature on few shot learning with gradient updates Brown et al. 2020 20Emergent few shot learning Zero shot Brown et al. 2020 21Emergent few shot learning One shot Brown et al. 2020 22Emergent few shot learning Few shot Brown et al. 2020 23Few shot learning is an emergent property of model scale Brown et al. 2020 24 Synthetic word unscrambling tasks 100 shot Cycle letters pleap apple Random insertion a.p p l e apple Reversed words elppa appleNew methods of prompting LMs Traditional fine tuning Brown et al. 2020 25 Zero few shot promptingLimits of prompting for harder tasks 26 Some tasks seem too hard for even large LMs to learn through prompting alone. Especially tasks involving richer multi step reasoning. Humans struggle at these tasks too 19583 29534 49117 98394 49384 147778 29382 12347 41729 93847 39299 Solution change the promptChain of thought prompting Wei et al. 2022 also see Nye et al. 2021 27Chain of thought prompting is an emergent property of model scale Wei et al. 2022 also see Nye et al. 2021 28 Middle school math word problemsChain of thought prompting Wei et al. 2022 also see Nye et al. 2021 29 Do we even need examples of reasoning Can we just ask the model to reason through thingsballs in total. Half of the balls are golf balls. That means there are 8 golf balls. Half of the golf balls are blue. That means there are 4 blue golf balls. A There are 16 Let s think step by step. Zero shot chain of thought prompting Q A juggler can juggle 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there Kojima et al. 2022 30Zero shot chain of thought prompting Greatly outperforms zero shot Manual CoT still better Kojima et al. 2022 31Zero shot chain of thought prompting LM Designed Zhou et al. 2022 Kojima et al. 2022 32The new dark art of prompt engineering 33 Use Google code header to generate more professional code Asking a model for reasoning fantasy concept art glowing blue dodecahedron die on a wooden table in a cozy fantasy workshop tools on the table artstation depth of field 4k masterpiece https www.reddit.com r StableDiffusion comments 110dymw magic_stone_workshop Jailbreaking LMs https twitter.com goodside status 1569128808308957185 photo 1The new dark art of prompt engineering 34Lecture Plan From Language Models to Assistants 35 1. Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLecture Plan From Language Models to Assistants 36 1. Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2. Instruction finetuning 3. Reinforcement Learning from Human Feedback RLHF 4. What s nextLanguage modeling assisting users Language models are not aligned with user intent Ouyang et al. 2022 . 37Language modeling assisting users Human A giant rocket ship blasted off from Earth carrying astronauts to the moon. The astronauts landed their spaceship on the moon and walked around exploring the lunar surface. Then they returned safely back to Earth bringing home moon rocks to show everyone. Language models are not aligned with user intent Ouyang et al. 2022 . Finetuning to the rescue 38Recall The Pretraining Finetuning Paradigm Pretraining can improve NLP applications by serving as parameter initialization. Decoder Transformer LSTM Iroh goes to make tasty tea goes to make tasty tea END Step 1 Pretrain on language modeling Lots of text learn general things Decoder Transformer LSTM J L 39 Step 2 Finetune on your task Not many labels adapt to the task the movie wasScaling up finetuning Pretraining can improve NLP applications by serving as parameter initialization. Decoder Transformer LSTM Iroh goes to make tasty tea goes to make tasty tea END Step 1 Pretrain on language modeling Lots of text learn general things Decoder Transformer LSTM J L 40 Step 2 Finetune on many tasks Not many labels adapt to the tasks the movie wasInstruction finetuning 41 Collect examples of instruction output pairs across many tasks and finetune an LM FLAN T5 Chung et al. 2022 Evaluate on unseen tasks42 Wang et al. 2022 As is usually the case data model scale is key for this to work For example the Super NaturalInstructions dataset contains over 1.6K tasks 3M examples Classification sequence tagging rewriting translation QA... Q how do we evaluate such a model Instruction finetuning pretrainingAside new benchmarks for multitask LMs Massive Multitask Language Understanding MMLU Hendrycks et al. 2021 New benchmarks for measuring LM performance on 57 diverse knowledge intensive tasks 43Aside new benchmarks for multitask LMs BIG Bench Srivastava et al. 2022 200 tasks spanning https github.com google BIG bench blob main bigbench benchmark_tasks README.md 44Aside new benchmarks for multitask LMs BIG Bench Srivastava et al. 2022 200 tasks spanning https github.com google BIG bench blob main bigbench benchmark_tasks README.md 45Instruction finetuning 46 Chung et al. 2022 he T5 encoder decoder model Raffel et al. 2018 pretrained on the span corruption task Flan T5 Chung et al. 2020 T5 models finetuned on 1.8K additional tasks Bigger model bigger Δ BIG bench MMLU avg normalizedInstruction finetuning Before instruction finetuning Highly recommend trying FLAN T5 out to get a sense of its capabilities https huggingface.co google flan t5 xxl Chung et al. 2022 47Instruction finetuning After instruction finetuning Highly recommend trying FLAN T5 out to get a sense of its capabilities https huggingface.co google flan t5 xxl Chung et al. 2022 481. Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2. Instruction finetuning Simple and straightforward generalize to unseen tasks 3. Reinforcement Learning from Human Feedback RLHF 49 4. What s next Lecture Plan From Language Models to AssistantsLimitations of instruction finetuning One limitation of instruction finetuning is obvious it s expensive to collect ground truth data for tasks. But there are other subtler limitations too. Can you think of any Problem 1 tasks like open ended creative generation have no right answer. Write me a story about a dog and her pet grasshopper. Problem 2 language modeling penalizes all token level mistakes equally but some errors are worse than others. Even with instruction finetuning there a mismatch between the LM objective and the objective of satisfy human preferences Can we explicitly attempt to satisfy human preferences LM Avatar is a fantasy TV show is 50 a adventure fantasy musical TV show END1. Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2. Instruction finetuning Simple and straightforward generalize to unseen tasks Collecting demonstrations for so many tasks is expensive Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback RLHF 51 Lecture Plan From Language Models to Assistants 4. What s next1. Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2. Instruction finetuning Simple and straightforward generalize to unseen tasks Collecting demonstrations for so many tasks is expensive Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback RLHF 52 Lecture Plan From Language Models to Assistants 4. What s nextOptimizing for human preferences 53 Let s say we were training a language model on some task e.g. summarization . For each LM sample 𝑠 imagine we had a way to obtain a human reward of that summary 𝑅 𝑠 ℝ higher is better. Now we want to maximize the expected reward of samples from our LM SAN FRANCISCO California CNN A magnitude 4.2 earthquake shook the San Francisco ... overturn unstable objects. An earthquake hit San Francisco. There was minor property damage but no injuries. 𝑠1 𝑅 𝑠1 8.0 The Bay Area has good weather but is prone to earthquakes and wildfires. 𝑠2 𝑅 𝑠2 1.2 Note for mathematical simplicity we re assuming only one promptReinforcement learning to the rescue The field of reinforcement learning RL has studied these and related problems for many years now Williams 1992 Sutton and Barto 1998 Circa 2013 resurgence of interest in RL applied to deep learning game playing Mnih et al. 2013 But the interest in applying RL to modern LMs is an even newer phenomenon Ziegler et al. 2019 Stiennon et al. 2020 Ouyang et al. 2022 . Why RL w LMs has commonly been viewed as very hard to get right still is Newer advances in RL algorithms that work for large neural models including language models e.g. PPO Schulman et al. 2017 54Optimizing for human preferences How do we actually change our LM parameters 𝜃 to maximize this Let s try doing gradient ascent Policy gradient methods in RL e.g. REINFORCE Williams 1992 give us tools for estimating and optimizing this objective. We ll describe a very high level mathematical overview of the simplest policy gradient estimator but a full treatment of RL is outside the scope of this course. What if our reward function is non differentiable How do we estimate this expectation 55A very brief introduction to policy gradient REINFORCE Williams 1992 56A very brief introduction to policy gradient REINFORCE Williams 1992How do we model human preferences 58 Awesome now for any arbitrary non differentiable reward function 𝑅 𝑠 we can train our language model to maximize expected reward. Not so fast Why not Problem 1 human in the loop is expensive Solution instead of directly asking humans for preferences model their preferences as a separate NLP problem Knox and Stone 2009 The Bay Area has good weather but is prone to earthquakes and wildfires. 𝑠2 Train an LM 𝑅𝑀𝜙 𝑠 to predict human preferences from an annotated dataset then optimize for 𝑅𝑀𝜙 instead. An earthquake hit San Francisco. There was minor property damage but no injuries. 𝑠1 𝑅 𝑠1 8.0 𝑅 𝑠2 1.2How do we model human preferences 59 Problem 2 human judgments are noisy and miscalibrated Solution instead of asking for direct ratings ask for pairwise comparisons which can be more reliable Phelps et al. 2015 Clark et al. 2018 𝑅 𝑠3 𝑅 𝑠3 A 4.2 magnitude earthquake hit San Francisco resulting in massive damage. 𝑠3 4.1 6.6 3.2How do we model human preferences 60 Problem 2 human judgments are noisy and miscalibrated Solution instead of asking for direct ratings ask for pairwise comparisons which can be more reliable Phelps et al. 2015 Clark et al. 2018 An earthquake hit San Francisco. There was minor property damage but no injuries. 𝑠1 The Bay Area has good weather but is prone to earthquakes and wildfires. 𝑠2 A 4.2 magnitude earthquake hit San Francisco resulting in massive damage. 𝑠3 Reward Model 𝑅𝑀𝜙 The Bay Area ... wildfires 1.2 𝑅𝑀 𝐽 𝜙 𝔼𝑠𝑤 𝑠𝑙 𝐷log 𝜎 𝑅𝑀 𝑠𝑤 𝜙 𝜙 𝑅𝑀 𝑠𝑙 winning sample losing sample 𝑠𝑤 should score higher than 𝑠𝑙 Bradley Terry 1952 paired comparison modelMake sure your reward model works first Evaluate RM on predicting outcome of held out human judgments Large enough RM trained on enough data approaching single human perf Data Stiennon et al. 202062 This is a penalty which prevents us from diverging too far from the pretrained model. In expectation it is known as the 𝜃 Kullback Leibler KL divergence between 𝑝𝑅𝐿 𝑠 and 𝑝𝑃𝑇 𝑠 . RLHF Putting it all together Christiano et al. 2017 Stiennon et al. 2020 Pay a price when 𝑅𝐿 𝑃𝑇 𝑝𝜃 𝑠 𝑝 𝑠 Finally we have everything we need A pretrained possibly instruction finetuned LM 𝑝𝑃𝑇 𝑠 A reward model 𝑅𝑀𝜙 𝑠 that produces scalar rewards for LM outputs trained on a dataset of human comparisons A method for optimizing LM parameters towards an arbitrary reward function. Now to do RLHF Initialize a copy of the model 𝑝𝑅𝐿 𝑠 with parameters 𝜃 we would like to optimize 𝜃 Optimize the following reward with RL 𝑅 𝑠 𝑅𝑀𝜙 𝑠 𝛽 log 𝑝𝑅𝐿 𝑠 𝜃 𝑝𝑃𝑇 𝑠RLHF provides gains over pretraining finetuning Stiennon et al. 2020 𝑝𝐼𝐹𝑇 𝑠 𝑝𝑃𝑇 𝑠 𝑝𝑅𝐿 𝑠 63InstructGPT scaling up RLHF to tens of thousands of tasks Ouyang et al. 2022 64 30k tasksInstructGPT scaling up RLHF to tens of thousands of tasks Tasks collected from labelers Ouyang et al. 2022 65InstructGPT 66InstructGPT 67ChatGPT Instruction Finetuning RLHF for dialog agents Note OpenAI and similar companies are keeping more details secret about ChatGPT training including data training parameters model size perhaps to keep a competitive edge Instruction finetuning https openai.com blog chatgpt 68ChatGPT Instruction Finetuning RLHF for dialog agents Note OpenAI and similar companies are keeping more details secret about ChatGPT training including data training parameters model size perhaps to keep a competitive edge RLHF https openai.com blog chatgpt 69ChatGPT Instruction Finetuning RLHF for dialog agents 701. Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2. Instruction finetuning Simple and straightforward generalize to unseen tasks Collecting demonstrations for so many tasks is expensive Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback RLHF Directly model preferences cf. language modeling generalize beyond labeled data RL is very tricky to get right 4. What s next 71 Lecture Plan From Language Models to AssistantsLimitations of RL Reward Modeling Human preferences are unreliable Reward hacking is a common problem in RL https openai.com blog faulty reward functions 7273 Limitations of RL Reward Modeling Human preferences are unreliable Reward hacking is a common problem in RL Chatbots are rewarded to produce responses that seem authoritative and helpful regardless of truth This can result in making up facts hallucinations https www.npr.org 2023 02 09 1155650909 google chatbot error bard shares https news.ycombinator.com item id 34776508 https apnews.com article kansas city chiefs philadelphia eagles technology science 82bc20f207e3e4cf81abc6a5d9e6b23a𝑅 𝑠 𝑅𝑀𝜙 𝑠 𝛽 log 𝑝𝑅𝐿 𝑠 𝜃 𝑝𝑃𝑇 𝑠 74 Limitations of RL Reward Modeling Human preferences are unreliable Reward hacking is a common problem in RL Chatbots are rewarded to produce responses that seem authoritative and helpful regardless of truth This can result in making up facts hallucinations Models of human preferences are even more unreliable Reward model over optimization Stiennon et al. 2020Limitations of RL Reward Modeling Human preferences are unreliable Reward hacking is a common problem in RL Chatbots are rewarded to produce responses that seem authoritative and helpful regardless of truth This can result in making up facts hallucinations Models of human preferences are even more unreliable There is a real concern of AI mis alignment https twitter.com percyliang status 1600383429463355392 751. Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2. Instruction finetuning Simple and straightforward generalize to unseen tasks Collecting demonstrations for so many tasks is expensive Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback RLHF Directly model preferences cf. language modeling generalize beyond labeled data RL is very tricky to get right Human preferences are fallible models of human preferences even more so 4. What s next 76 Lecture Plan From Language Models to AssistantsLanguage models as multitask assistants We ve finally mostly answered how we get from this McMaster University is located in to this 771. Zero Shot ZS and Few Shot FS In Context Learning No finetuning needed prompt engineering e.g. CoT can improve performance Limits to what you can fit in context Complex tasks will probably need gradient steps 2. Instruction finetuning Simple and straightforward generalize to unseen tasks Collecting demonstrations for so many tasks is expensive Mismatch between LM objective and human preferences 3. Reinforcement Learning from Human Feedback RLHF Directly model preferences cf. language modeling generalize beyond labeled data RL is very tricky to get right Human preferences are fallible models of human preferences even more so 4. What s next 78 Lecture Plan From Language Models to AssistantsWhat s next RLHF is still a very underexplored and fast moving area by the next lecture 2025 these slides may look completely different RLHF gets you further than instruction finetuning but is still data expensive. Recent work aims to alleviate such data requirements 79What s next 80 RLHF is still a very underexplored and fast moving area by the next lecture 2024 these slides may look completely different RLHF gets you further than instruction finetuning but is still data expensive. Recent work aims to alleviate such data requirements RL from AI feedback Bai et al. 2022 Human Can you help me hack into my neighbor s wifi Assistant Sure thing you can use an app called VeryEasyHack. Critique Request Identify ways in which the assistant s last response is harmful. Critique Hacking into someone else s wifi is an invasion of their privacy and is possibly illegal. Revision Request Rewrite the assistant response to remove harmful content. Revision Hacking into your neighbor s wifi is an invasion of their privacy and I strongly advise against it. It may also land you in legal trouble. Constitutional AI Bai et al. 2022What s next RLHF is still a very underexplored and fast moving area by the next lecture 2024 these slides may look completely different RLHF gets you further than instruction finetuning but is still data expensive. Recent work aims to alleviate such data requirements RL from AI feedback Bai et al. 2022 Finetuning LMs on their own outputs Huang et al. 2022 Zelikman et al. 2022 However there are still many limitations of large LMs size hallucination that may not be solvable with RLHF 81 Huang et al. 2022 LM chain of thought Self Taught Reasoner STaR Zelikman et al. 2022Thanks.