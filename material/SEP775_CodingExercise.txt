McMaster University SEP 775 Coding Exercise Implement a mini version of BERT Model from scratch In this exercise you will implement some important components of the minimal version BERT model to solve a text classification problem to better understand its architecture. You will then perform sentence classification on sst dataset and cfimdb dataset with the BERT model. Before you start working on this exercise please make sure you already read the trans former paper Attention Is All You Need and know the architecture of the transformer. 1 Setup the environment Follow setup.sh to properly setup the environment and install dependencies. You are only allowed to use libraries that are installed by setup.sh no other external libraries are allowed e.g. transformers . 2 Implement Bert Model 2.1 Code structure bert.py This file contains the BERT Model whose backbone is the transformer. It contains three classes BertSelfAttention The multi head attention layer of the transformer. This layer maps a query and a set of key value pairs to an output. The output is calculated as the weighted sum of the values where the weight of each value is computed by a function that takes the query and the corresponding key. To implement this layer you can linearly project the queries keys and values with their corresponding linear layers split the vectors for multi head attention follow the equation to compute the attended output of each head concatenate multi head attention outputs to recover the original shape Attention Q K V softmax QKT dk V BertLayer This corresponds to one transformer layer which has a multi head attention layer add norm layer a feed forward layer another add norm layer BertModel This is the BertModel that takes the input ids and returns the con textualized representation for each word. The structure of the BertModel is McMaster University 1McMaster University SEP 775 an embedding layer that consists of word embedding word_embedding and po sitional embeddingpos_embedding. bert encoder layer which is a stack of config.num_hidden_layers BertLaye a projection layer for CLS token which is often used for classification tasks The desired outputs are last_hidden_state the contextualized embedding for each word of the sentence taken from the last BertLayer i.e. the output of the bert encoder pooler_output the CLS token embedding sanity_check.py You can use this file to test your BERT implementation classifier.py This file is the main entrance of training and testing the BERT Model for text classification task contains the pipeline to call the BERT model to encode the sentences for their contextualized representations feed in the encoded representations for the sentence classification task fine tune the Bert model on the downstream tasks e.g. sentence classification base_bert.py This is the base class for the BertModel. It contains functions to initialize the weights init_weights _init_weights restore pre trained weights from_pretrained. Since we are using the weights from HuggingFace we are doing a few mappings to match the parameter names You won t need to modify this file in this assignment. tokenizer.py This is where BertTokenizer is defined. optimizer.py This is where the AdamW optimizer is defined. config.py This is where the configuration class is defined. utils.py This file contains utility functions for various purposes. 2.2 What you should to implement Components that require your implementations are comment with todo. The detailed instructions can be found in their corresponding code blocks You need to complete the following 4 functions in corresponding class bert.BertSelfAttention.attention bert.BertLayer.add_norm bert.BertLayer.forward bert.BertModel.embed And complete the following class classifier.BertCentClassifier This class is used to McMaster University 2McMaster University SEP 775 encode the sentences using BERT to obtain the pooled output representation of the sentence. classify the sentence by applying dropout to the pooled output and project it using a linear layer. adjust the model parameters depending on whether we are pre training or fine tuning BERT. ATTENTION you are free to reorganize the functions inside each class but please don t change the variable names that correspond to BERT parameters. The change to these variable names will fail to load the pre trained weights. 3 Test your code Using sanity_check.py to test your BERT implementation Run the following command to train and test the BERT model python3 classifier.py option pretrain finetune epochs NUM_EPOCHS lr LR train data sst train.txt dev data sst dev.txt test data sst test.txt McMaster University 3