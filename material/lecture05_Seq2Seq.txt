Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Seq to Seq Models and Neural Machine TranslationLSTM LSTM LSTM LSTM LSTM s LSTM LSTM LSTM LSTM argmax argmax argmax argmax argmax s One Type of Conditional Language Model I hate this movie kono eiga ga kirai hate this movie Encoder I DecoderMachine Translation Machine Translation MT is the task of translating a sentence x from one language the source language to a sentence y in another language the target language . x L homme est n√© libre et partout il est dans les fers y Man is born free but everywhere he is in chains RousseauThe early history of MT 1950s Machine translation research began in the early 1950s on machines less powerful than high school calculators before term A.I. coined Concurrent with foundational work on automata formal languages probabilities and information theory MT heavily funded by military but basically just simple rule based systems doing word substitution Human language is more complicated than that and varies more across languages Little understanding of natural language syntax semantics pragmatics Problem soon appeared intractable 1 minute video showing 1954 MT https youtu.be K HfpsHPmvwThe early history of MT 1950s1990s 2010s Statistical Machine Translation Core idea Learn a probabilistic model from data Suppose we re translating French English. We want to find best English sentence y given French sentence x Use Bayes Rule to break this down into two components to be learned separately Translation Model Models how words and phrases should be translated fidelity . Learned from parallel data. Language Model Models how to write good English fluency . Learned from monolingual data.What happens in translation isn t trivial to model 1519Âπ¥600ÂêçË•øÁè≠Áâô‰∫∫Âú®Â¢®Ë•øÂì•ÁôªÈôÜ ÂéªÂæÅÊúçÂá†Áôæ‰∏á‰∫∫Âè£ ÁöÑÈòøÂÖπÁâπÂÖãÂ∏ùÂõΩ ÂàùÊ¨°‰∫§Èîã‰ªñ‰ª¨ÊçüÂÖµ‰∏âÂàÜ‰πã‰∫å In 1519 six hundred Spaniards landed in Mexico to conquer the Aztec Empire with a population of a few million. They lost two thirds of their soldiers in the first clash. translate.google.com 2009 1519 600 Spaniards landed in Mexico millions of people to conquer the Aztec empire the first two thirds of soldiers against their loss. translate.google.com 2013 1519 600 Spaniards landed in Mexico to conquer the Aztec empire hundreds of millions of people the initial confrontation loss of soldiers two thirds. translate.google.com 2015 1519 600 Spaniards landed in Mexico millions of people to conquer the Aztec empire the first two thirds of the loss of soldiers they clash.1990s 2010s Statistical Machine Translation SMT was a huge research field The best systems were extremely complex Hundreds of important details Systems had many separately designed subcomponents Lots of feature engineering Need to design features to capture particular language phenomena Required compiling and maintaining extra resources Like tables of equivalent phrases Lots of human effort to maintain Repeated effort for each language pairWhat is Neural Machine Translation Neural Machine Translation NMT is a way to do Machine Translation with a single end to end neural network The neural network architecture is called a sequence to sequence model aka seq2seq and it involves two RNNs LSTMsEncoder RNN Neural Machine Translation NMT ST ART he hit me with a pie Source sentence input il a m entart√© Decoder RNN Encoder RNN produces an encoding of the source sentence. The sequence to sequence model Target sentence output Encoding of the source sentence. Provides initial hidden state for Decoder RNN. Decoder RNN is a Language Model that generates target sentence conditioned on encoding. argmax argmax argmax Note This diagram shows test time behavior decoder output is fed in as next step s input he hit me with a pie END argmax argmax argmax argmaxSequence to sequence is versatile The general notion here is an encoder decoder model One neural network takes input and produces a neural representation Another network produces output based on that neural representation If the input and output are sequences we call it a seq2seq model Sequence to sequence is useful for more than just MT Many NLP tasks can be phrased as sequence to sequence Summarization long text short text Dialogue previous utterances next utterance Parsing input text output parse as sequence Code generation natural language Python codeNeural Machine Translation NMT The sequence to sequence model is an example of a Conditional Language Model Language Model because the decoder is predicting the next word of the target sentence y Conditional because its predictions are also conditioned on the source sentence x NMT directly calculates Probability of next target word given target words so far and source sentence x Question How to train an NMT system Easy Answer Get a big parallel corpus But there is now exciting work on unsupervised NMT data augmentation etc.Training a Neural Machine Translation system Encoder RNN il a m entart√© Source sentence from corpus a pie START he hit me with Target sentence from corpus Seq2seq is optimized as a single system. Backpropagation operates end to end . Decoder RNN ùë¶ 1 ùë¶ 2 ùë¶ 3 ùë¶ ùë¶ 5 ùë¶ ùë¶ 7 ùêΩ1 ùêΩ2 ùêΩ3 ùêΩ ùêΩ5 ùêΩ ùêΩ7 negative log prob of he negative log prob of END negative log prob of withMulti layer deep encoder decoder machine translation net Die Proteste waren am Wochenende eskaliert EOS The protests escalated over the weekend 0.2 0.6 0.1 0.7 0.1 0.4 0.6 0.2 0.3 0.4 0.2 0.3 0.1 0.4 0.2 0.2 0.4 0.1 0.5 0.2 0.4 0.2 0.3 0.4 0.2 0.2 0.6 0.1 0.7 0.1 0.2 0.6 0.1 0.7 0.1 0.2 0.6 0.1 0.7 0.1 0.1 0.3 0.1 0.7 0.1 0.2 0.6 0.1 0.3 0.1 0.4 0.5 0.5 0.4 0.1 0.2 0.6 0.1 0.7 0.1 0.2 0.6 0.1 0.7 0.1 0.2 0.2 0.1 0.1 0.1 0.2 0.6 0.1 0.7 0.1 0.1 0.3 0.1 0.7 0.1 0.2 0.6 0.1 0.4 0.1 0.2 0.8 0.1 0.5 0.1 0.2 0.6 0.1 0.7 0.1 0.4 0.6 0.1 0.7 0.1 0.2 0.6 0.1 0.3 0.1 0.1 0.6 0.1 0.3 0.1 0.2 0.4 0.1 0.2 0.1 0.3 0.6 0.1 0.5 0.1 0.2 0.6 0.1 0.7 0.1 0.2 0.1 0.1 0.7 0.1 0.1 0.3 0.1 0.4 0.2 0.2 0.6 0.1 0.7 0.1 0.4 0.4 0.3 0.2 0.3 0.5 0.5 0.9 0.3 0.2 0.2 0.6 0.1 0.5 0.1 0.1 0.6 0.1 0.7 0.1 0.2 0.6 0.1 0.7 0.1 0.3 0.6 0.1 0.7 0.1 0.4 0.4 0.1 0.7 0.1 0.2 0.6 0.1 0.7 0.1 0.4 0.6 0.1 0.7 0.1 0.3 0.5 0.1 0.7 0.1 0.2 0.6 0.1 0.7 0.1 The protests escalated over the weekend EOS Encoder Builds up sentence meaning Source sentence Translation generated Feeding in last word Decoder Sutskever et al. 2014 Luong et al. 2015 The hidden states from RNN layer i are the inputs to RNN layer i 1Decoding Greedy decoding We saw how to generate or decode the target sentence by taking argmax on each step of the decoder he hit me with a pie END START he hit me with a pie This is greedy decoding take most probable word on each step argmax argmax argmax argmax 15 argmax argmax argmaxProblems with greedy decoding 16 Greedy decoding has no way to undo decisions Input il a m entart√© he he hit he hit a he hit me with a pie whoops no going back now How to fix thisExhaustive search decoding Ideally we want to find a length T translation y that maximizes We could try computing all possible sequences y This means that on each step t of the decoder we re tracking Vt possible partial translations where V is vocab size This O VT complexity is far too expensive 17Beam search decoding Core idea On each step of decoder keep track of the k most probable partial translations which we call hypotheses k is the beam size in practice around 5 to 10 in NMT A hypothesis has a score which is its log probability Scores are all negative and higher score is better We search for high scoring hypotheses tracking top k on each step Beam search is not guaranteed to find optimal solution But much more efficient than exhaustive search 18Beam search decoding example Beam size k 2. Blue numbers START Calculate prob dist of next word 19Beam search decoding example Beam size k 2. Blue numbers START he I 0.7 log PLM he START 0.9 log PLM I START Take top k words and compute scores 20Beam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 10 0.7 0.9 2.9 log PLM struck START he 0.7 1.6 log PLM was START I 0.9 1.8 log PLM got START I 0.9 1.7 log PLM hit START he 0.7 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got START he I 11 0.7 0.9 1.6 1.8 1.7 2.9 Of these k2 hypotheses just keep k with highest scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck START he I 12 0.7 0.9 1.6 1.8 1.7 2.9 2.5 log PLM me START he hit 1.7 2.9 log PLM hit START I was 1.6 2.8 log PLM a START he hit 1.7 3.8 log PLM struck START I was 1.6 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck START he I 13 0.7 0.9 1.6 1.8 1.7 2.9 2.5 2.8 3.8 2.9 Of these k2 hypotheses just keep k with highest scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck tart pie with on START he I 14 0.7 0.9 1.6 1.8 1.7 2.9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck tart pie with on START he I 15 0.7 0.9 1.6 1.8 1.7 2.9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 Of these k2 hypotheses just keep k with highest scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck tart pie with on in with a one START he I 16 0.7 0.9 1.6 1.8 1.7 2.9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 4.5 3.7 4.3 4.8 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck tart pie with on in with a one START he I 17 0.7 0.9 1.6 1.8 1.7 2.9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 4.5 3.7 4.3 4.8 Of these k2 hypotheses just keep k with highest scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck tart pie with on in with a one pie tart pie tart START he I 18 0.7 0.9 1.6 1.8 1.7 2.9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 4.5 3.7 4.3 4.8 4.3 4.6 5.0 5.3 For each of the k hypotheses find top k next words and calculate scoresBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck tart pie with on in with a one pie tart pie tart START he I 19 0.7 0.9 1.6 1.8 1.7 2.9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 4.5 3.7 4.3 4.8 4.3 4.6 5.0 5.3 This is the top scoring hypothesisBeam search decoding example Beam size k 2. Blue numbers hit struck was got a me hit struck tart pie with on in with a one pie tart pie tart START he I 20 0.7 0.9 1.6 1.8 1.7 2.9 2.5 2.8 3.8 2.9 3.5 3.4 3.3 4.0 4.5 3.7 4.3 4.8 4.3 4.6 5.0 5.3 Backtrack to obtain the full hypothesisHow do we evaluate Machine Translation BLEU Bilingual Evaluation Understudy BLEU compares the machine written translation to one or several human written translation s and computes a similarity score based on n gram precision usually for 1 2 3 and 4 grams Plus a penalty for too short system translations BLEU is useful but imperfect There are many valid ways to translate a sentence So a good translation can get a poor BLEU score because it has low n gram overlap with the human translation L Source BLEU a Method for Automatic Evaluation of Machine Translation Papineni et al 2002. http aclweb.org anthology P02 1040Reference translation 1 The U.S. island of Guam is maintaining a high state of alert after the Guam airport and its offices both received an e mail from someone calling himself the Saudi Arabian Osama bin Laden and threatening a biological chemical attack against public places such as the airport . Reference translation 3 The US International Airport of Guam and its office has received an email from a self claimed Arabian millionaire named Laden which threatens to launch a biochemical attack on such public places as airport . Guam authority has been on alert . Reference translation 4 US Guam International Airport and its office received an email from Mr. Bin Laden and other rich businessman from Saudi Arabia . They said there would be biochemistry air raid to Guam Airport and other public places . Guam needs to be in high precaution about this matter . Reference translation 2 Guam International Airport and its offices are maintaining a high state of alert after receiving an e mail that was from a person claiming to be the wealthy Saudi Arabian businessman Bin Laden and that threatened to launch a biological and chemical attack on the airport and other public places . Machine translation The American international airport and its the office all receives one calls self the sand Arab rich business and so on electronic mail which sends out The threat will be able after public place and so on the airport to start the biochemistry attack highly alerts after the maintenance. Reference translation 1 The U.S. island of Guam is maintaining a high state of alert after the Guam airport and its offices both received an e mail from someone calling himself the Saudi Arabian Osama bin Laden and threatening a biological chemical attack against public places such as the airport . Reference translation 3 The US International Airport of Guam and its office has received an email from a self claimed Arabian millionaire named Laden which threatens to launch a biochemical attack on such public places as airport . Guam authority has been on alert . Reference translation 4 US Guam International Airport and its office received an email from Mr. Bin Laden and other rich businessman from Saudi Arabia . They said there would be biochemistry air raid to Guam Airport and other public places . Guam needs to be in high precaution about this matter . Reference translation 2 Guam International Airport and its offices are maintaining a high state of alert after receiving an e mail that was from a person claiming to be the wealthy Saudi Arabian businessman Bin Laden and that threatened to launch a biological and chemical attack on the airport and other public places . Machine translation The American international airport and its the office all receives one calls self the sand Arab rich business and so on electronic mail which sends out The threat will be able after public place and so on the airport to start the biochemistry attack highly alerts after the maintenance. BLEU score against 4 reference translations Papineni et al. 200245 40 35 30 25 20 15 10 5 0 2013 2014 2015 2016 2017 2018 2019 Sources http www.meta net.eu events meta forum 2016 slides 09_sennrich.pdf http matrix.statmt.org Phrase based SMT Syntax based SMT Neural MT MT progress over time Edinburgh En De WMT newstest2013 Cased BLEU NMT 2015 from U. Montr√©al NMT 2019 FAIR on newstest2019Advantages of NMT Compared to SMT NMT has many advantages Better performance More fluent Better use of context Better use of phrase similarities A single neural network to be optimized end to end No subcomponents to be individually optimized Requires much less human engineering effort No feature engineering Same method for all language pairsDisadvantages of NMT Compared to SMT NMT is less interpretable Hard to debug NMT is difficult to control For example can t easily specify rules or guidelines for translation Safety concernsNMT the first big success story of NLP Deep Learning Neural Machine Translation went from a fringe research attempt in 2014 to the leading standard method in 2016 2014 First seq2seq paper published 2016 Google Translate switches from SMT to NMT and by 2018 everyone has This is amazing SMT systems built by hundreds of engineers over many years outperformed by NMT systems trained by a small group of engineers in a few monthsSummary so far Lots of new information today and last week What are some of the practical takeaways 1. LSTMs are powerful 2. Clip your gradients 3. Use bidirectionality when possible Die Proteste waren am Wochenende eskaliert EOS The protests escalated over the weekend 0 2 0 . 6 0 1 0 0 . . 4 0 . 6 0 . . 2 0 . . . 2 0 . 3 0 . 1 0 . . 2 0 . 4 0 . 1 0 . 5 7 3 4 0 0 0 0 . . . . 1 4 . 4 0 0 . 3 0 . 4 0 . 0 7 0 . 1 0 6 1 0 0 0 0 0 1 0 . 7 0 . 0 2 . 6 0 . . . 1 0 . 3 0 1 0 0 . . 4 0 . 5 0 . 5 . 2 0 . 6 0 . 1 0 0 . . 4 7 0 0 . . 0 0 . . . . . 2 2 2 2 0 0 0 0 1 0 . 3 . . . . . 2 6 6 6 0 0 0 0 . . . . 1 1 1 0 0 0 . . . . 7 7 7 0 . . . 1 1 1 2 0 . 2 0 . 1 . . 2 0 . 6 0 . 1 0 0 . . 1 7 0 0 . . 1 1 0 7 . 1 0 0 . . 1 2 0 0 . . 3 6 0 0 . . 0 4 . 1 . 2 0 . 8 0 0 0 0 . 5 0 . 1 0 . 2 0 . 6 0 1 1 0 . 7 0 . 1 0 . 4 0 0 1 1 0 . 2 0 6 6 0 . 1 0 . 3 0 . 1 0 1 . 6 0 . 1 0 0 3 . 1 0 0 . . 2 0 . 4 0 . 1 0 2 0 . 1 0 0 . . . . 1 1 3 2 0 0 6 6 0 0 . . 0 0 . . . . 5 7 0 0 . . 1 1 . 2 0 . . . 1 0 . . . . . . 1 1 1 0 0 . . 7 7 0 0 . . 1 0 . 1 0 . 3 0 . 1 0 . 4 0 . 2 0 2 0 . 6 0 . 1 0 . 7 0 . 1 0 . 2 . . 3 9 0 . 3 . . . . 4 5 2 4 5 6 0 0 0 . 1 0 . 5 0 0 0 0 . 6 0 . 1 0 . 7 0 3 0 6 0 7 1 4 0 . 4 0 . 1 0 0 0 7 0 . 1 0 0 0 . . . 2 4 3 0 0 0 . . . 6 6 5 0 0 0 . . . 1 1 1 0 0 0 . . . 7 7 7 0 0 0 . . . 1 1 1 0 0 0 . . . . 2 2 1 0 0 0 0 0 . . . . . . . 6 6 0 0 . . . 1 1 1 0 . . . . 7 7 0 0 0 0 . . . . . . . 3 2 1 1 1 1 The protests escalated over the weekend EOS Encoder Builds up sentence meaning Source sentence Translation generated Feeding in last word Decoder Conditioning Bottleneck 4. Encoder Decoder Neural Machine Translation Systems work very well2. Why attention Sequence to sequence the bottleneck problem Encoder RNN Source sentence input START he hit me with a pie il a m entart√© he hit me with a pie END Decoder RNN Encoding of the source sentence. Target sentence output Problems with this architecture 391. Why attention Sequence to sequence the bottleneck problem Encoder RNN Source sentence input START he hit me with a pie il a m entart√© he hit me with a pie END Decoder RNN Target sentence output Encoding of the source sentence. This needs to capture all information about the source sentence. Information bottleneck 40Attention Attention provides a solution to the bottleneck problem. Core idea on each step of the decoder use direct connection to the encoder to focus on a particular part of the source sequence First we will show via diagram no equations then we will show with equations 41Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention scores Core idea on each step of the decoder use direct connection to the encoder to focus on a particular part of the source sequence dot product Source sentence input 42Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention scores dot product Source sentence input 43Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention scores dot product Source sentence input 44Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention scores dot product Source sentence input 45Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention scores On this decoder timestep we re mostly focusing on the first encoder hidden state he Attention distribution Take softmax to turn the scores into a probability distribution Source sentence input 46Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention distribution Attention scores Attention output Use the attention distribution to take a weighted sum of the encoder hidden states. The attention output mostly contains information from the hidden states that received high attention. Source sentence input 47Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention distribution Attention scores Attention output Concatenate attention output with decoder hidden state then use to compute ùë¶ 1 as before ùë¶ .1 he Source sentence input 48Sequence to sequence with attention Encoder RNN START he il a m entart√© Decoder RNN Attention scores Attention distribution Attention output ùë¶2 hit Sometimes we take the attention output from the previous step and also feed it into the decoder along with the usual decoder input . Source sentence input 49Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention scores Attention distribution Attention output he hit ùë¶3 me Source sentence input 50Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention scores Attention distribution Attention output he hit me ùë¶4 with Source sentence input 51Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention scores Attention distribution Attention output he hit me with ùë¶5 a Source sentence input 52Sequence to sequence with attention Encoder RNN START il a m entart√© Decoder RNN Attention scores Attention distribution Attention output he hit me with a ùë¶6 pie Source sentence input 53Attention in equations We have encoder hidden states On timestep t we have decoder hidden state We get the attention scores for this step We take softmax to get the attention distribution sums to 1 for this step this is a probability distribution and We use to take a weighted sum of the encoder hidden states to get the attention output Finally we concatenate the attention output with the decoder hidden state and proceed as in the non attention seq2seq model 54Attention is great Attention significantly improves NMT performance It s very useful to allow decoder to focus on certain parts of the source Attention provides a more human like model of the MT process You can look back at the source sentence while translating rather than needing to remember it all Attention solves the bottleneck problem Attention allows decoder to look directly at source bypass bottleneck Attention helps with the vanishing gradient problem Provides shortcut to faraway states Attention provides some interpretability By inspecting attention distribution we see what the decoder was focusing on We get soft alignment for free This is cool because we never explicitly trained an alignment system The network just learned alignment by itself he hit me with a pie il a m entart√© 55There are several attention variants We have some values and a query Attention always involves 1. Computing the attention scores 2. Taking softmax to get attention distribution ùò¢ 3. Using attention distribution to take weighted sum of values thus obtaining the attention output a sometimes called the context vector There are multiple ways to do this 56Attention variants There are several ways you can compute Basic dot product attention from and Note this assumes . This is the version we saw earlier. Multiplicative attention Luong Pham and Manning 2015 Where is a weight matrix. Perhaps better called bilinear attention Reduced rank multiplicative attention ùëíùëñ ùë†ùëáùëºùëáùëΩ‚Ñéùëñ ùëºùë† ùëá ùëΩ‚Ñéùëñ For low rank matrices ùëº ‚Ñùùëò ùëë2 ùëΩ ‚Ñùùëò ùëë1 ùëò ùëë ùëë 1 2 Additive attention Bahdanau Cho and Bengio 2014 Where are weight matrices and is a weight vector. d3 the attention dimensionality is a hyperparameter Additive is a weird bad name. It s really using a feed forward neural net layer. More information Deep Learning for NLP Best Practices Ruder 2017. http ruder.io deep learning nlp best practices index.html attention Massive Exploration of Neural Machine Translation Architectures Britz et al 2017 https arxiv.org pdf 1703.03906.pdf You ll think about the relative advantages disadvantages of these in Assignment 4 Remember this when we look at Transformers next week 57Attention is a general Deep Learning technique 58 We ve seen that attention is a great way to improve the sequence to sequence model for Machine Translation. However You can use attention in many architectures not just seq2seq and many tasks not just MT More general definition of attention Given a set of vector values and a vector query attention is a technique to compute a weighted sum of the values dependent on the query. We sometimes say that the query attends to the values. For example in the seq2seq attention model each decoder hidden state query attends to all the encoder hidden states values .Attention is a general Deep Learning technique 59 More general definition of attention Given a set of vector values and a vector query attention is a technique to compute a weighted sum of the values dependent on the query. Intuition The weighted sum is a selective summary of the information contained in the values where the query determines which values to focus on. Attention is a way to obtain a fixed size representation of an arbitrary set of representations the values dependent on some other representation the query . Upshot Attention has become the powerful flexible general way pointer and memory manipulation in all deep learning models. A new idea from after 2010 From NMT