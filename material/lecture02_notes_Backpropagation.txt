Forward computation. Backward propagation. Neuron Units. Max margin Loss. Gradient checks. Xavier parameter initialization. Learning rates. Adagrad. This set of notes introduces single and multilayer neural networks and how they can be used for classiﬁcation purposes. We then dis cuss how they can be trained using a distributed gradient descent technique known as backpropagation. We will see how the chain rule can be used to make parameter updates sequentially. After a rigorous mathematical discussion of neural networks we will discuss some practical tips and tricks in training neural networks involving neuron units non linearities gradient checks Xavier parameter ini tialization learning rates Adagrad etc. Lastly we will motivate the use of recurrent neural networks as a language model. 1 Neural Networks Foundations Figure 1 We see here how a non linear decision boundary separates the data very well. This is the prowess of neural networks. Fun Fact Neural networks are biologically in spired classiﬁers which is why they are often called artiﬁcial neural networks to distinguish them from the organic kind. However in reality human neural networks are so much more capable and complex from artiﬁcial neural net works that it is usually better to not draw too many parallels between the two. We established in our previous discussions the need for non linear classiﬁers since most data are not linearly separable and thus our classiﬁcation performance on them is limited. Neural networks are a family of classiﬁers with non linear decision boundary as seen in Figure 1. Now that we know the sort of decision boundaries neural networks create let us see how they manage doing so. 1.1 A Neuron A neuron is a generic computational unit that takes n inputs and produces a single output. What differentiates the outputs of different neurons is their parameters also referred to as their weights . One of the most popular choices for neurons is the sigmoid or binary lo gistic regression unit. This unit takes an n dimensional input vector x and produces the scalar activation output a. This neuron is also associated with an n dimensional weight vector w and a bias scalar b. The output of this neuron is then a 1 1 exp wTx b We can also combine the weights and bias term above to equiva Neuron A neuron is the fundamental building block of neural networks. We will see that a neuron can be one of many functions that allows for non linearities to accrue in the network. lently formulate a 1 1 exp wT b x 1 Figure 2 This image captures how in a sigmoid neuron the input vector x is ﬁrst scaled summed added to a bias unit and then passed to the squashing sigmoid function. This formulation can be visualized in the manner shown in Fig ure 2. 1.2 A Single Layer of Neurons We extend the idea above to multiple neurons by considering the case where the input x is fed as an input to multiple such neurons as shown in Figure 3. If we refer to the different neurons weights as w 1 w m and the biases as b1 bm we can say the respective activations are a1 am a1 1 1 exp w 1 Tx b1 . . . am 1 1 exp w m Tx bm Let us deﬁne the following abstractions to keep the notation simple and useful for more complex networks σ z 1 1 exp z1 . . . 1 1 exp zm b b1 . . . bm Rm W w 1 T w m T Rm n We can now write the output of scaling and biases as Figure 3 This image captures how multiple sigmoid units are stacked on the right all of which receive the same input x. z Wx b The activations of the sigmoid function can then be written as a 1 . . . a m σ z σ Wx b So what do these activations really tell us Well one can think of these activations as indicators of the presence of some weighted combination of features. We can then use a combination of these activations to perform classiﬁcation tasks. 1.3 Feed forward Computation So far we have seen how an input vector x Rn can be fed to a layer of sigmoid units to create activations a Rm. But what is the intuition behind doing so Let us consider the following named entity recognition NER problem in NLP as an example Museums in Paris are amazing Dimensions for a single hidden layer neural network If we represent each word using a 4 dimensional word vector and we use a 5 word window as input then the input x R20. If we use 8 sigmoid units in the hidden layer and generate 1 score output from the activations then W R8 20 b R8 U R8 1 s R. The stage wise feed forward computation is then z Wx b a σ z s UTa Here we want to classify whether or not the center word Paris is a named entity. In such cases it is very likely that we would not just want to capture the presence of words in the window of word vectors but some other interactions between the words in order to make the classiﬁcation. For instance maybe it should matter that Museums is the ﬁrst word only if in is the second word. Such non linear de cisions can often not be captured by inputs fed directly to a Softmax function but instead require the scoring of the intermediate layer discussed in Section 1.2. We can thus use another matrix U Rm 1 to generate an unnormalized score for a classiﬁcation task from the activations s UTa UT f Wx b where f is the activation function. Figure 4 This image captures how a simple feed forward network might compute its output. Analysis of Dimensions If we represent each word using a 4 dimensional word vector and we use a 5 word window as input as in the above example then the input x R20. If we use 8 sigmoid units in the hidden layer and generate 1 score output from the activa tions then W R8 20 b R8 U R8 1 s R. 1.4 Maximum Margin Objective Function Like most machine learning models neural networks also need an optimization objective a measure of error or goodness which we want to minimize or maximize respectively. Here we will discuss a popular error metric known as the maximum margin objective. The idea behind using this objective is to ensure that the score computed for true labeled data points is higher than the score computed for false labeled data points. Using the previous example if we call the score computed for the true labeled window Museums in Paris are amazing as s and the score computed for the false labeled window Not all museums in Paris as sc subscripted as c to signify that the window is corrupt . Then our objective function would be to maximize s sc or to minimize sc s . However we modify our objective to ensure that error is only computed if sc s sc s 0. The intuition behind doing this is that we only care the the true data point have a higher score than the false data point and that the rest does not matter. Thus we want our error to be sc s if sc s else 0. Thus our optimization objective is now minimize J max sc s 0 However the above optimization objective is risky in the sense that it does not attempt to create a margin of safety. We would want the true labeled data point to score higher than the false labeled data point by some positive margin . In other words we would want error to be calculated if s sc and not just when s sc 0 . Thus we modify the optimization objective minimize J max sc s 0 We can scale this margin such that it is 1 and let the other parameters in the optimization problem adapt to this without any change in performance. For more information on this read about functional and geometric margins a topic often covered in the study of Support Vector Machines. Finally we deﬁne the following opti The max margin objective function is most commonly associated with Support Vector Machines SVMs mization objective which we optimize over all training windows minimize J max 1 sc s 0 In the above formulation sc UT f Wxc b and s UT f Wx b . 1.5 Training with Backpropagation Elemental In this section we discuss how we train the different parameters in the model when the cost J discussed in Section 1.4 is positive. No parameter updates are necessary if the cost is 0. Since we typically update parameters using gradient descent or a variant such as SGD we typically need the gradient information for any parameter as required in the update equation θ t 1 θ t α θ t J Backpropagation is technique that allows us to use the chain rule of differentiation to calculate loss gradients for any parameter used in the feed forward computation on the model. To understand this further let us understand the toy network shown in Figure 5 for which we will perform backpropagation. Figure 5 This is a 4 2 1 neural network where neuron j on layer k receives input z k j and produces activation output a k j . Here we use a neural network with a single hidden layer and a single unit output. Let us establish some notation that will make it easier to generalize this model later xi is an input to the neural network. s is the output of the neural network. Each layer including the input and output layers has neurons which receive an input and produce an output. The j th neuron of layer k receives the scalar input z k j and produces the scalar activation output a k j . We will call the backpropagated error calculated at z k j as δ k j . Layer 1 refers to the input layer and not the ﬁrst hidden layer. For the input layer xj z 1 j a 1 j . W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th Thus W 1 W and W 2 U to put this new generalized notation in perspective of Section 1.3. Backpropagation Notation xi is an input to the neural network. s is the output of the neural net work. The j th neuron of layer k receives the scalar input z k j and produces the scalar activation output a k j . For the input layer xj z 1 j a 1 j . W k is the transfer matrix that maps the output from the k th layer to the input to the k 1 th. Thus W 1 W and W 2 UT using notation from Section 1.3. Let us begin Suppose the cost J 1 sc s is positive and we want to perform the update of parameter W 1 14 in Figure 5 and Figure 6 we must realize that W 1 14 only contributes to z 2 1 and thus a 2 1 . This fact is crucial to understanding backpropagation backpropagated gradients are only affected by values they contribute to. a 2 1 is consequently used in the forward computation of score by multiplication with W 2 1 . We can see from the max margin loss that J s J sc 1 Therefore we will work with s W 1 ij here for simplicity. Thus s W 1 ij W 2 a 2 W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i W 1 ij W 2 i a 2 i z 2 i z 2 i W 1 ij W 2 i f z 2 i z 2 i z 2 i W 1 ij W 2 i f z 2 i z 2 i W 1 ij W 2 i f z 2 i W 1 ij b 1 i a 1 1 W 1 i1 a 1 2 W 1 i2 a 1 3 W 1 i3 a 1 4 W 1 i4 W 2 i f z 2 i W 1 ij b 1 i k a 1 k W 1 ik W 2 i f z 2 i a 1 j δ 2 i a 1 j We see above that the gradient reduces to the product δ 2 i a 1 j where δ 2 i is essentially the error propagating backwards from the i th neu ron in layer 2. a 1 j is an input fed to i th neuron in layer 2 when scaled by Wij. Figure 6 This subnetwork shows the relevant parts of the network required to update W 1 ij Let us discuss the error sharing distribution interpretation of backpropagation better using Figure 6 as an example. Say we were to update W 1 14 1. We start with the an error signal of 1 propagating backwards from a 3 1 . 2. We then multiply this error by the local gradient of the neuron which maps z 3 1 to a 3 1 . This happens to be 1 in this case and thus the error is still 1. This is now known as δ 3 1 1. 3. At this point the error signal of 1 has reached z 3 1 . We now need to distribute the error signal so that the fair share of the error reaches to a 2 1 . 4. This amount is the error signal at z 3 1 δ 3 1 W 2 1 W 2 1 . Thus the error at a 2 1 W 2 1 . 5. As we did in step 2 we need to move the error across the neuron which maps z 2 1 to a 2 1 . We do this by multiplying the error signal at a 2 1 by the local gradient of the neuron which happens to be f z 2 1 . 6. Thus the error signal at z 2 1 is f z 2 1 W 2 1 . This is known as δ 2 1 . 7. Finally we need to distribute the fair share of the error to W 1 14 by simply multiplying it by the input it was responsible for for warding which happens to be a 1 4 . 8. Thus the gradient of the loss with respect to W 1 14 is calculated to be a 1 4 f z 2 1 W 2 1 . Notice that the result we arrive at using this approach is exactly the same as that we arrived at using explicit differentiation earlier. Thus we can calculate error gradients with respect to a parameter in the network using either the chain rule of differentiation or using an error sharing and distributed ﬂow approach both of these ap proaches happen to do the exact same thing but it might be helpful to think about them one way or another. Bias Updates Bias terms such as b 1 1 are mathematically equivalent to other weights contributing to the neuron input z 2 1 as long as the input being forwarded is 1. As such the bias gradients for neuron i on layer k is simply δ k i . For instance if we were updating b 1 1 instead of W 1 14 above the gradient would simply be f z 2 1 W 2 1 . Generalized steps to propagate δ k to δ k 1 Figure 7 Propagating error from δ k to δ k 1 1. We have error δ k i propagating backwards from z k i i.e. neuron i at layer k. See Figure 7. 2. We propagate this error backwards to a k 1 j by multiplying δ k i by the path weight W k 1 ij . 3. Thus the error received at a k 1 j is δ k i W k 1 ij . 4. However a k 1 j may have been forwarded to multiple nodes in the next layer as shown in Figure 8. It should receive responsibility for errors propagating backward from node m in layer k too using the exact same mechanism. 5. Thus error received at a k 1 j is δ k i W k 1 ij δ k m W k 1 mj . 6. In fact we can generalize this to be i δ k i W k 1 ij . 7. Now that we have the correct error at a k 1 j we move it across neuron j at layer k 1 by multiplying with with the local gradient f z k 1 j . 8. Thus the error that reaches z k 1 j called δ k 1 j is f z k 1 j i δ k i W k 1 ij Figure 8 Propagating error from δ k to δ k 1 1.6 Training with Backpropagation Vectorized So far we discussed how to calculate gradients for a given parameter in the model. Here we will generalize the approach above so that we update weight matrices and bias vectors all at once. Note that these are simply extensions of the above model that will help build intuition for the way error propagation can be done at a matrix vector level. For a given parameter W k ij we identiﬁed that the error gradient is simply δ k 1 i a k j . As a reminder W k is the matrix that maps a k to z k 1 . We can thus establish that the error gradient for the entire matrix W k is W k δ k 1 1 a k 1 δ k 1 1 a k 2 δ k 1 2 a k 1 δ k 1 2 a k 2 . . . . . . ... δ k 1 a k T Error propagates from layer k 1 to k in the following manner δ k f z k W k Tδ k 1 Of course this assumes that in the forward propagation the signal z k ﬁrst goes through activation neurons f to generate activations a k and are then linearly combined to yield z k 1 via transfer matrix W k . Thus we can write an entire matrix gradient using the outer prod uct of the error vector propagating into the matrix and the activations forwarded by the matrix. Now we will see how we can calculate the error vector δ k . We established earlier using Figure 8 that δ k j f z k j i δ k 1 i W k ij . This can easily generalize to matrices such that δ k f z k W k Tδ k 1 In the above formulation the operator corresponds to an element wise product between elements of vectors RN RN RN . Computational efﬁciency Having explored element wise updates as well as vector wise updates we must realize that the vectorized implementations run substantially faster in scientiﬁc computing environments such as MATLAB or Python using NumPy SciPy packages . Thus we should use vectorized implementation in prac tice. Furthermore we should also reduce redundant calculations in backpropagation for instance notice that δ k depends directly on δ k 1 . Thus we should ensure that when we update W k using δ k 1 we save δ k 1 to later derive δ k and we then repeat this for k 1 . . . 1 . Such a recursive procedure is what makes backpropa gation a computationally affordable procedure. 2 Neural Networks Tips and Tricks Having discussed the mathematical foundations of neural networks we will now dive into some tips and tricks commonly employed when using neural networks in practice. 2.1 Gradient Check In the last section we discussed in detail how to calculate error gradients updates for parameters in a neural network model via calculus based analytic methods. Here we now introduce a tech nique of numerically approximating these gradients though too computationally inefﬁcient to be used directly for training the net works this method will allow us to very precisely estimate the derivative with respect to any parameter it can thus serve as a useful sanity check on the correctness of our analytic derivatives. Given a model with parameter vector θ and loss function J the numerical gradient around θi is simply given by centered difference formula f θ J θ i J θ i 2ϵ where ϵ is a small number usually around 1e 5 . The term J θ i is simply the error calculated on a forward pass for a given input when we perturb the parameter θ s ith element by ϵ. Similarly the term J θ i is the error calculated on a forward pass for the same input when we perturb the parameter θ s ith element by ϵ. Thus using two forward passes we can approximate the gradient with respect to any given parameter element in the model. We note that this deﬁnition of the numerical gradient follows very naturally from the deﬁnition of the derivative where in the scalar case f x f x ϵ f x ϵ Gradient checks are a great way to compare analytical and numerical gradients. Analytical gradients should be close and numerical gradients can be calculated using f θ J θ i J θ i 2ϵ J θ i and J θ i can be evalu ated using two forward passes. An implementation of this can be seen in Snippet 2.1. Of course there is a slight difference the deﬁnition above only perturbs x in the positive direction to compute the gradient. While it would have been perfectly acceptable to deﬁne the numerical gradi ent in this way in practice it is often more precise and stable to use the centered difference formula where we perturb a parameter in both directions. The intuition is that to get a better approximation of the derivative slope around a point we need to examine the func tion f s behavior both to the left and right of that point. It can also be shown using Taylor s theorem that the centered difference formula has an error proportional to ϵ2 which is quite small whereas the derivative deﬁnition is more error prone. Now a natural question you might ask is if this method is so pre cise why do we not use it to compute all of our network gradients instead of applying back propagation The simple answer as hinted earlier is inefﬁciency recall that every time we want to compute the gradient with respect to an element we need to make two forward passes through the network which will be computationally expen sive. Furthermore many large scale neural networks can contain millions of parameters and computing two passes per parameter is clearly not optimal. And since in optimization techniques such as SGD we must compute the gradients once per iteration for several thousands of iterations it is obvious that this method quickly grows intractable. This inefﬁciency is why we only use gradient check to verify the correctness of our analytic gradients which are much quicker to compute. A standard implementation of gradient check is shown below Snippet 2.1 def eval_numerical_gradient f x a naive implementation of numerical gradient of f at x f should be a function that takes a single argument x is the point numpy array to evaluate the gradient at fx f x evaluate function value at original point grad np.zeros x.shape h 0.00001 iterate over all indexes in x it np.nditer x flags multi_index op_flags readwrite while not it.finished evaluate function at x h ix it.multi_index old_value x ix x ix old_value h increment by h fxh_left f x evaluate f x h x ix old_value h decrement by h fxh_right f x evaluate f x h x ix old_value restore to previous value very important compute the partial derivative grad ix fxh_left fxh_right 2 h the slope it.iternext step to next dimension return grad 2.2 Regularization As with many machine learning models neural networks are highly prone to overﬁtting where a model is able to obtain near perfect per formance on the training dataset but loses the ability to generalize to unseen data. A common technique used to address overﬁtting an issue also known as the high variance problem is the incorpora tion of an L2 regularization penalty. The idea is that we will simply append an extra term to our loss function J so that the overall cost is now calculated as JR J λ L i 1 W i F The Frobenius Norm of a matrix U is deﬁned as follows U F s i j U2 ij In the above formulation W i F is the Frobenius norm of the matrix W i the i th weight matrix in the network and λ is the hyper parameter controlling how much weight the regularization term has relative to the original cost function. Since we are trying to minimize JR what regularization is essentially doing is penaliz ing weights for being too large while optimizing over the original cost function. Due to the quadratic nature of the Frobenius norm which computes the sum of the squared elements of a matrix L2 regularization effectively reduces the ﬂexibility of the model and thereby reduces the overﬁtting phenomenon. Imposing such a con straint can also be interpreted as the prior Bayesian belief that the optimal weights are close to zero how close depends on the value of λ. Choosing the right value of λ is critical and must be chosen via hyperparameter tuning. Too high a value of λ causes most of the weights to be set too close to 0 and the model does not learn anything meaningful from the training data often obtaining poor ac curacy on training validation and testing sets. Too low a value and we fall into the domain of overﬁtting once again. It must be noted that the bias terms are not regularized and do not contribute to the cost term above try thinking about why this is the case There are indeed other types of regularization that are sometimes used such as L1 regularization which sums over the absolute values rather than squares of parameter elements however this is less commonly applied in practice since it leads to sparsity of parameter weights. In the next section we discuss dropout which effectively acts as another form of regularization by randomly dropping i.e. setting to zero neurons in the forward pass. 2.3 Dropout Dropout is a powerful technique for regularization ﬁrst introduced by Srivastava et al. in Dropout A Simple Way to Prevent Neural Net works from Overﬁtting. The idea is simple yet effective during train ing we will randomly drop with some probability 1 p a subset of neurons during each forward backward pass or equivalently we will keep alive each neuron with a probability p . Then during testing we will use the full network to compute our predictions. The result is that the network typically learns more meaningful informa tion from the data is less likely to overﬁt and usually obtains higher performance overall on the task at hand. One intuitive reason why this technique should be so effective is that what dropout is doing is essentially doing is training exponentially many smaller networks at once and averaging over their predictions. Dropout applied to an artiﬁcial neural network. Image credits to Srivastava et al. In practice the way we introduce dropout is that we take the out put h of each layer of neurons and keep each neuron with prob ability p and else set it to 0. Then during back propagation we only pass gradients through neurons that were kept alive during the forward pass. Finally during testing we compute the forward pass using all of the neurons in the network. However a key sub tlety is that in order for dropout to work effectively the expected output of a neuron during testing should be approximately the same as it was during training else the magnitude of the outputs could be radically different and the behavior of the network is no longer well deﬁned. Thus we must typically divide the outputs of each neuron during testing by a certain value it is left as an exercise to the reader to determine what this value should be in order for the expected outputs during training and testing to be equivalent. 2.4 Neuron Units So far we have discussed neural networks that contain sigmoidal neurons to introduce nonlinearities however in many applications better networks can be designed using other activation functions. Some common choices are listed here with their function and gra dient deﬁnitions and these can be substituted with the sigmoidal functions discussed above. Sigmoid This is the default choice we have discussed the activation function σ is given by Figure 9 The response of a sigmoid nonlinearity σ z 1 1 exp z where σ z 0 1 The gradient of σ z is σ z exp z 1 exp z σ z 1 σ z Tanh The tanh function is an alternative to the sigmoid function that is often found to converge faster in practice. The primary differ ence between tanh and sigmoid is that tanh output ranges from 1 to 1 while the sigmoid ranges from 0 to 1. Figure 10 The response of a tanh nonlinearity tanh z exp z exp z exp z exp z 2σ 2z 1 where tanh z 1 1 The gradient of tanh z is tanh z 1 exp z exp z exp z exp z 2 1 tanh2 z Hard tanh The hard tanh function is sometimes preferred over the tanh function since it is computationally cheaper. It does however saturate for magnitudes of z greater than 1. The activation of the hard tanh is Figure 11 The response of a hard tanh nonlinearity hardtanh z 1 z 1 z 1 z 1 1 z 1 The derivative can also be expressed in a piecewise functional form hardtanh z 1 1 z 1 0 otherwise Soft sign The soft sign function is another nonlinearity which can be considered an alternative to tanh since it too does not saturate as easily as hard clipped functions Figure 12 The response of a soft sign nonlinearity softsign z z 1 z The derivative is the expressed as softsign z sgn z 1 z 2 where sgn is the signum function which returns 1 depending on the sign of z ReLU The ReLU Rectiﬁed Linear Unit function is a popular choice of activation since it does not saturate even for larger values of z and has found much success in computer vision applications Figure 13 The response of a ReLU nonlinearity rect z max z 0 The derivative is then the piecewise function rect z 1 z 0 0 otherwise Leaky ReLU Traditional ReLU units by design do not propagate any error for non positive z the leaky ReLU modiﬁes this such that a small error is allowed to propagate backwards even when z is negative Figure 14 The response of a leaky ReLU nonlinearity leaky z max z k z where 0 k 1 This way the derivative is representable as leaky z 1 z 0 k otherwise 2.5 Data Preprocessing As is the case with machine learning models generally a key step to ensuring that your model obtains reasonable performance on the task at hand is to perform basic preprocessing on your data. Some common techniques are outlined below. Mean Subtraction Given a set of input data X it is customary to zero center the data by subtracting the mean feature vector of X from X. An important point is that in practice the mean is calculated only across the training set and this mean is subtracted from the training validation and testing sets. Normalization Another frequently used technique though perhaps less so than mean subtraction is to scale every input feature dimension to have similar ranges of magnitudes. This is useful since input features are often measured in different units but we often want to initially consider all features as equally important. The way we accomplish this is by simply dividing the features by their respective standard deviation calculated across the training set. Whitening Not as commonly used as mean subtraction normalization whiten ing essentially converts the data to a have an identity covariance matrix that is features become uncorrelated and have a variance of 1. This is done by ﬁrst mean subtracting the data as usual to get X . We can then take the Singular Value Decomposition SVD of X to get matrices U S V. We then compute UX to project X into the basis deﬁned by the columns of U. We ﬁnally divide each dimension of the result by the corresponding singular value in S to scale our data appropriately if a singular value is zero we can just divide by a small number instead . 2.6 Parameter Initialization A key step towards achieving superlative performance with a neu ral network is initializing the parameters in a reasonable way. A good starting strategy is to initialize the weights to small random numbers normally distributed around 0 and in practice this often words acceptably well. However in Understanding the dif ficulty of training deep feedforward neural networks 2010 Xavier et al study the effect of different weight and bias initialization schemes on training dynamics. The empirical ﬁndings suggest that for sigmoid and tanh activation units faster convergence and lower error rates are achieved when the weights of a matrix W Rn l 1 n l are initialized randomly with a uniform distribution as follows W U s 6 n l n l 1 s 6 n l n l 1 Where n l is the number of input units to W fan in and n l 1 is the number of output units from W fan out . In this parameter initialization scheme bias units are initialized to 0. This approach attempts to maintain activation variances as well as backpropagated gradient variances across layers. Without such initialization the gradient variances which are a proxy for information generally decrease with backpropagation across layers. 2.7 Learning Strategies The rate magnitude of model parameter updates during training can be controlled using the learning rate. In the following naive Gradient Descent formulation α is the learning rate θnew θold α θ Jt θ You might think that for fast convergence rates we should set α to larger values however faster convergence is not guaranteed with larger convergence rates. In fact with very large learning rates we might experience that the loss function actually diverges because the parameters update causes the model to overshoot the convex minima as shown in Figure 15. In non convex models most of those we work with the outcome of a large learning rate is unpredictable but the chances of diverging loss functions are very high. Figure 15 Here we see that updating parameter w2 with a large learning rate can lead to divergence of the error. The simple solution to avoiding a diverging loss is to use a very small learning rate so that we carefully scan the parameter space of course if we use too small a learning rate we might not converge in a reasonable amount of time or might get caught in local minima. Thus as with any other hyperparameter the learning rate must be tuned effectively. Since training is the most expensive phase in a deep learning system some research has attempted to improve this naive approach to setting learning learning rates. For instance Ronan Collobert scales the learning rate of a weight Wij where W Rn l 1 n l by the inverse square root of the fan in of the neuron n l . There are several other techniques that have proven to be effec tive as well one such method is annealing where after several iterations the learning rate is reduced in some way this method ensures that we start off with a high learning rate and approach a minimum quickly as we get closer to the minimum we start lower ing our learning rate so that we can ﬁnd the optimum under a more ﬁne grained scope. A common way to perform annealing is to reduce the learning rate α by a factor x after every n iterations of learning. Exponential decay is also common where the learning rate α at iter ation t is given by α t α0e kt where α0 is the initial learning rate and k is a hyperparameter. Another approach is to allow the learning rate to decrease over time such that α t α0τ max t τ In the above scheme α0 is a tunable parameter and represents the starting learning rate. τ is also a tunable parameter and represents the time at which the learning rate should start reducing. In practice this method has been found to work quite well. In the next section we discuss another method for adaptive gradient descent which does not require hand set learning rates. 2.8 Momentum Updates Momentum methods a variant of gradient descent inspired by the study of dynamics and motion in physics attempt to use the veloc ity of updates as a more effective update scheme. Pseudocode for momentum updates is shown below Snippet 2.2 Computes a standard momentum update on parameters x v mu v alpha grad_x x v 2.9 Adaptive Optimization Methods AdaGrad is an implementation of standard stochastic gradient de scent SGD with one key difference the learning rate can vary for each parameter. The learning rate for each parameter depends on the history of gradient updates of that parameter in a way such that parameters with a scarce history of updates are updated faster using a larger learning rate. In other words parameters that have not been updated much in the past are likelier to have higher learning rates now. Formally θt i θt 1 i α q t τ 1 g2 τ i gt i where gt i θt i Jt θ In this technique we see that if the RMS of the history of gradients is extremely low the learning rate is very high. A simple implemen tation of this technique is Snippet 2.3 Assume the gradient dx and parameter vector x cache dx 2 x learning_rate dx np.sqrt cache 1e 8 Other common adaptive methods are RMSProp and Adam whose update rules are shown below courtesy of Andrej Karpathy Snippet 2.4 Update rule for RMS prop cache decay_rate cache 1 decay_rate dx 2 x learning_rate dx np.sqrt cache eps Snippet 2.5 Update rule for Adam m beta1 m 1 beta1 dx v beta2 v 1 beta2 dx 2 x learning_rate m np.sqrt v eps RMSProp is a variant of AdaGrad that utilizes a moving average of squared gradients in particular unlike AdaGrad its updates do not become monotonically smaller. The Adam update rule is in turn a variant of RMSProp but with the addition of momentum like updates. We refer the reader to the respective sources of these methods for more detailed analyses of their behavior.