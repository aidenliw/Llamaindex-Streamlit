Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Overview of NLP and Word VectorsLecture Plan 2 Lecture 1 Introduction and Word Vectors 1. The course 2. Human language and word meaning 3. Word2vec introduction 4. Word2vec objective function gradients 5. Optimization basics 6. Looking at word vectors Key learning today The astounding result that word meaning can be represented rather well by a high dimensional vector of real numbersCourse logistics in brief 3 Instructor Hamidreza Mahyar Head TA Reza Namazi Time Wednesday 3 30pm 6 30pm We put a lot of other important information on Avenue to Learn. Please read itWhat do we hope to teach A.k.a. learning goals 4 1. The foundations of the effective modern methods for deep learning applied to NLP Basics first then key methods used in NLP in 2024 Word vectors feed forward networks recurrent networks attention encoder decoder models transformers large pre trained language models etc. 2. A big picture understanding of human languages and the difficulties in understanding and producing them via computers 3. An understanding of and ability to build systems in PyTorch for some of the major problems in NLP Word meaning dependency parsing machine translation question answeringCourse work and grading policy 5 4 Assignments 20 4 Quizzes 10 Midterm 35 Final Group Project 1 3 people 34 Late day policy NO free late days 1 off course grade per day late Assignments not accepted more than 3 days late unless given permission in advance Collaboration policy Understand allowed collaboration and how to document it Don t take code off the web acknowledge working with other students write your own assignment solutionsTrained on text data neural machine translation is quite good https kiswahili.tuko.co.keThe SEC said Musk your tweets are a blight. They really could cost you your job if you don t stop all this tweeting at night. Then Musk cried Why The tweets I wrote are not mean I don t use all caps and I m sure that my tweets are clean. But your tweets can move markets and that s why we re sore. You may be a genius and a billionaire but it doesn t give you the right to be a bore S I broke the window. Q What did I break S I gracefully saved the day. Q What did I gracefully save S I gave John flowers. Q Who did I give flowers to S I gave her a rose and a guitar. Q Who did I give a rose and a guitar to How many users have signed up since the start of 2020 SELECT count id FROM users WHERE created_at 2020 01 01 What is the average number of influencers each user is subscribed to SELECT avg count FROM SELECT user_id count FROM subscribers GROUP BY user_id AS avg_subscriptions_per_user GPT 3 A first step on the path to foundation modelsChatGPT A recent intriguing set of capabilitiesChatGPT A recent intriguing set of capabilitiesChatGPT A recent intriguing set of capabilitiesHow do we represent the meaning of a word Definition meaning Webster dictionary the idea that is represented by a word phrase etc. the idea that a person wants to express by using words signs etc. the idea that is expressed in a work of writing art etc. Commonest linguistic way of thinking of meaning signifier symbol signified idea or thing denotational semantics tree 13How do we have usable meaning in a computer Previously commonest NLP solution Use e.g. WordNet a thesaurus containing lists of synonym sets and hypernyms is a relationships e.g. synonym sets containing good e.g. hypernyms of panda Synset procyonid.n.01 Synset carnivore.n.01 Synset placental.n.01 Synset mammal.n.01 Synset vertebrate.n.01 Synset chordate.n.01 Synset animal.n.01 Synset organism.n.01 Synset living_thing.n.01 Synset whole.n.02 Synset object.n.01 Synset physical_entity.n.01 Synset entity.n.01 noun good noun good goodness noun good goodness noun commodity trade_good good adj good adj sat full good adj good adj sat estimable good honorable respectable adj sat beneficial good adj sat good adj sat good just upright adverb well good adverb thoroughly soundly good 14 from nltk.corpus import wordnet as wn poses n noun v verb s adj s a adj r adv for synset in wn.synsets good print .format poses synset.pos .join l.name for l in synset.lemmas from nltk.corpus import wordnet as wn panda wn.synset panda.n.01 hyper lambda s s.hypernyms list panda.closure hyperProblems with resources like WordNet 15 A useful resource but missing nuance e.g. proficient is listed as a synonym for good This is only correct in some contexts Also WordNet list offensive synonyms in some synonym sets without any coverage of the connotations or appropriateness of words Missing new meanings of words e.g. wicked badass nifty wizard genius ninja bombest Impossible to keep up to date Subjective Requires human labor to create and adapt Can t be used to accurately compute word similarity see following slidesRepresenting words as discrete symbols In traditional NLP we regard words as discrete symbols hotel conference motel a localist representation Such symbols for words can be represented by one hot vectors motel 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 hotel 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 Vector dimension number of words in vocabulary e.g. 500 000 Means one 1 the rest 0s 16Problem with words as discrete symbols 17 Example in web search if a user searches for Seattle motel we would like to match documents containing Seattle hotel But motel 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 hotel 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 These two vectors are orthogonal There is no natural notion of similarity for one hot vectors Solution Could try to rely on WordNet s list of synonyms to get similarity But it is well known to fail badly incompleteness etc. Instead learn to encode similarity in the vectors themselves Sec. 9.2.2Representing words by their context 21 Distributional semantics A word s meaning is given by the words that frequently appear close by You shall know a word by the company it keeps J. R. Firth 1957 11 One of the most successful ideas of modern statistical NLP When a word w appears in a text its context is the set of words that appear nearby within a fixed size window . We use the many contexts of w to build up a representation of w government debt problems turning into banking crises as happened in 2009 saying that Europe needs unified banking regulation to replace the hodgepodge India has just given its banking system a shot in the arm These context words will represent bankingWord vectors We will build a dense vector for each word chosen so that it is similar to vectors of words that appear in similar contexts measuring similarity as the vector dot scalar product Note word vectors are also called word embeddings or neural word representations They are a distributed representation banking 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 monetary 0.413 0.582 0.007 0.247 0.216 0.718 0.147 0.051 19Word meaning as a neural word vector visualization 0.286 0.792 0.177 0.107 0.109 0.542 0.349 0.271 0.487 expect 203. Word2vec Overview 21 Word2vec Mikolov et al. 2013 is a framework for learning word vectors Idea We have a large corpus body of text a long list of words Every word in a fixed vocabulary is represented by a vector Go through each position t in the text which has a center word c and context outside words o Use the similarity of the word vectors for c and o to calculate the probability of o given c or vice versa Keep adjusting the word vectors to maximize this probabilityWord2Vec Overview Example windows and process for computing 洧녞洧녻洧노 洧녱 洧녻洧노 crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 洧녞洧녻洧노 1 洧녻洧노 22 洧녞洧녻洧노 2 洧녻洧노 洧녞洧녻洧노 1 洧녻洧노 洧녞洧녻洧노 2 洧녻洧노Word2Vec Overview Example windows and process for computing 洧녞洧녻洧노 洧녱 洧녻洧노 crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 洧녞洧녻洧노 2 洧녻洧노 洧녞洧녻洧노 1 洧녻洧노 23 洧녞洧녻洧노 2 洧녻洧노 洧녞洧녻洧노 1 洧녻洧노Word2vec objective function For each position 洧노 1 洧녢 predict context words within a window of fixed size m given center word 洧녻洧노. Data likelihood The objective function 洧냫洧랚is the average negative log likelihood Minimizing objective function Maximizing predictive accuracy sometimes called a cost or loss function 24Word2vec objective function We want to minimize the objective function Question How to calculate 洧녞洧녻洧노 洧녱 洧녻洧노 洧랚 Answer We will use two vectors per word w 洧녺洧녻when w is a center word 洧녹洧녻when w is a context word Then for a center word c and a context word o 25Word2Vec with Vectors 洧녞洧녹洧녷洧洧녶洧녪洧녳洧뉧롐뛿롐 洧녺洧녰洧녵洧노洧녶 Example windows and process for computing 洧녞洧녻洧노 洧녱 洧녻洧노 short for P 洧녷洧洧녶洧녪洧녳洧뉧롐뛿롐 洧녰洧녵洧노洧녶 洧녹洧녷洧洧녶洧녪洧녳洧뉧롐뛿롐 洧녺洧녰洧녵洧노洧녶 洧랚 crises banking into turning problems as center word at position t outside context words in window of size 2 outside context words in window of size 2 洧녞洧녹洧녪洧녩洧녵洧녲洧녰洧녵洧녮 洧녺洧녰洧녵洧노洧녶 洧녞洧녹洧녫洧洧녰洧멇롐뒳롐 洧녺洧녰洧녵洧노洧녶 洧녞洧녹洧녷洧洧녶洧녪洧녳洧뉧롐뛿롐 洧녺洧녰洧녵洧노洧녶 洧녞洧녹洧노洧녹洧녵洧녰洧녵洧녮 洧녺洧녰洧녵洧노洧녶 All words vectors 洧랚 appear in denominator 26Word2vec prediction function The softmax function maps arbitrary values 洧논洧녰to a probability distribution 洧녷洧녰 max because amplifies probability of largest 洧논洧녰 soft because still assigns some probability to smaller 洧논洧녰 Frequently used in Deep Learning But sort of a weird name because it returns a distribution 27To train the model Optimize value of parameters to minimize loss To train a model we gradually adjust parameters to minimize a loss Recall 洧랚represents all the model parameters in one long vector In our case with d dimensional vectors and V many words we have 칟 Remember every word has two vectors We optimize these parameters by walking down the gradient see right figure We compute all vector gradients 284. 293031325. Optimization Gradient Descent We have a cost function 洧냫洧랚we want to minimize Gradient Descent is an algorithm to minimize 洧냫洧랚 Idea for current value of 洧랚 calculate gradient of 洧냫洧랚 then take small step in direction of negative gradient. Repeat. Note Our objectives may not be convex like this L But life turns out to be okay J 33Update equation in matrix notation Update equation for single parameter Algorithm Gradient Descent 洧띺 step size or learning rate 35Stochastic Gradient Descent Problem 洧냫洧랚 So is a function of all windows in the corpus potentially billions is very expensive to compute You would wait a very long time before making a single update Very bad idea for pretty much all neural nets Solution Stochastic gradient descent SGD Repeatedly sample windows and update after each one Algorithm 36Word2vec maximizes objective function bc putting similar words nearbc in space 37How to evaluate word vectors 38 Related to general evaluation in NLP Intrinsic vs. extrinsic Intrinsic Evaluation on a specific intermediate subtask Fast to compute Helps to understand that system Not clear if really helpful unless correlation to real task is established Extrinsic Evaluation on a real task Can take a long time to compute accuracy Unclear if the subsystem is the problem or its interaction or other subsystems If replacing exactly one subsystem with another improves accuracy  WinningMeaning similaritc Another intrinsic word vector evaluation 39 Word vector distances and their correlation with human judgments Example dataset WordSim353 http www.cs.technion.ac.il gabr resources data wordsim353 Word 1 Word 2 Human mean tiger cat 7.35 tiger tiger 10 book paper 7.46 computer internet 7.58 plane car 5.77 professor doctor 6.62 stock phone 1.62 stock CD 1.31 stock jaguar 0.92Classification review and notation 40 Supervised learning we have a training dataset consisting of samples xi yi Ni 1 xi are inputs e.g. words indices or vectors sentences documents etc. Dimension d yi are labels one of C classes we try to predict for example classes sentiment named entities buy sell decision other words later multi word sequencesNeural classification Typical ML stats softmax classifier Learned parameters 풪 are just elements of W not input representation x which has sparse symbolic features Classifier gives linear decision boundary which can be limiting compose our data multiple times giving a non linear classifier A neural network classifier differs in that We learn both W and distributed representations for words The word vectors x re represent one hot vectors moving them around in an intermediate layer vector space for easy classification with a linear softmax classifier Conceptually we have an embedding layer x Le We use deep networks more layers that let us re represent and But typically it is linear relative to the pre final 41 layer representationSoftmax classifier Again we can tease apart the prediction function into three steps 1. For each row y of W calculate dot product with x 2. Apply softmax function to get normalized probability softmax 洧녭 洧녽 3. Choose the y with maximum probability For each training example x y our objective is to maximize the probability of the correct class y or we can minimize the negative log probability of that class 4243 Thanks.