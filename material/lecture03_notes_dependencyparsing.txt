Keyphrases Dependency Parsing. 1 Dependency Grammar and Dependency Structure Parse trees in NLP analogous to those in compilers are used to ana lyze the syntactic structure of sentences. There are two main types of structures used constituency structures and dependency structures. Constituency Grammar uses phrase structure grammar to organize words into nested constituents. This will be covered in more detail in following chapters. We now focus on Dependency Parsing. Dependency structure of sentences shows which words depend on modify or are arguments of which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head or governor superior regent to the dependent or modiﬁer inferior subordinate . Usually these dependencies form a tree structure. They are often typed with the name of grammatical relations subject prepositional object apposition etc. . An example of such a dependency tree is shown in Figure 1. Sometimes a fake ROOT node is added as the head to the whole tree so that every word is a dependent of exactly one node. Figure 1 Dependency tree for the sen tence Bills on ports and immigration were submitted by Senator Brownback Republican of Kansas 1.1 Dependency Parsing Dependency parsing is the task of analyzing the syntactic depen dency structure of a given input sentence S. The output of a depen dency parser is a dependency tree where the words of the input sen tence are connected by typed dependency relations. Formally the de pendency parsing problem asks to create a mapping from the input sentence with words S w0w1...wn where w0 is the ROOT to its dependency tree graph G. Many different variations of dependency based methods have been developed in recent years including neural network based methods which we will describe later. To be precise there are two subproblems in dependency parsing adapted from Kuebler et al. chapter 1.2 1. Learning Given a training set D of sentences annotated with de pendency graphs induce a parsing model M that can be used to parse new sentences. 2. Parsing Given a parsing model M and a sentence S derive the optimal dependency graph D for S according to M. 1.2 Transition Based Dependency Parsing Transition based dependency parsing relies on a state machine which deﬁnes the possible transitions to create the mapping from the input sentence to the dependency tree. The learning problem is to induce a model which can predict the next transition in the state machine based on the transition history. The parsing problem is to construct the optimal sequence of transitions for the input sentence given the previously induced model. Most transition based systems do not make use of a formal grammar. 1.3 Greedy Deterministic Transition Based Parsing This system was introduced by Nivre in 2003 and was radically dif ferent from other methods in use at that time. This transition system is a state machine which consists of states and transitions between those states. The model induces a sequence of transitions from some initial state to one of several terminal states. States For any sentence S w0w1...wn a state can be described with a triple c σ β A 1. a stack σ of words wi from S 2. a buffer β of words wi from S 3. a set of dependency arcs A of the form wi r wj where wi wj are from S and r describes a dependency relation. It follows that for any sentence S w0w1...wn 1. an initial state c0 is of the form w0 σ w1 ... wn β only the ROOT is on the stack σ all other words are in the buffer β and no actions have been chosen yet 2. a terminal state has the form σ β A . Transitions Figure 2 Transitions for Dependency Parsing. There are three types of transitions between states 1. Shift Remove the ﬁrst word in the buffer and push it on top of the stack. Pre condition buffer has to be non empty. 2. Left Arcr Add a dependency arc wj r wi to the arc set A where wi is the word second to the top of the stack and wj is the word at the top of the stack. Remove wi from the stack. Pre condition the stack needs to contain at least two items and wi cannot be the ROOT. 3. Right Arcr Add a dependency arc wi r wj to the arc set A where wi is the word second to the top of the stack and wj is the word at the top of the stack. Remove wj from the stack. Pre condition The stack needs to contain at least two items. A more formal deﬁnition of these three transitions is presented in Figure 2. 1.4 Neural Dependency Parsing While there are many deep models for dependency parsing this section focuses speciﬁcally on greedy transition based neural de pendency parsers. This class of model has demonstrated compara ble performance and signiﬁcantly better efﬁciency than traditional feature based discriminative dependency parsers. The primary dis tinction from previous models is the reliance on dense rather than sparse feature representations. The model we will describe employs the arc standard system for transitions as presented in section 1.3. Ultimately the aim of the model is to predict a transition sequence from some initial con ﬁguration c to a terminal conﬁguration in which the dependency parse tree is encoded. As the model is greedy it attempts to correctly predict one transition T shift Left Arcr Right Arcr at a time based on features extracted from the current conﬁguration c σ β A . Recall σ is the stack β the buffer and A the set of dependency arcs for a given sentence. Feature Selection Depending on the desired complexity of the model there is ﬂexi bility in deﬁning the input to the neural network. The features for a given sentence S generally include some subset of 1. Sword Vector representations for some of the words in S and their dependents at the top of the stack σ and buffer β. 2. Stag Part of Speech POS tags for some of the words in S. POS tags comprise a small discrete set P NN NNP NNS DT JJ ... 3. Slabel The arc labels for some of the words in S. The arc labels comprise a small discrete set describing the dependency relation L amod tmod nsubj csubj dobj ... For each feature type we will have a corresponding embedding ma trix mapping from the feature s one hot encoding to a d dimensional dense vector representation. The full embedding matrix for Sword is Ew Rd Nw where Nw is the dictionary vocabulary size. Corre spondingly the POS and label embedding matrices are Et Rd Nt and El Rd Nl where Nt and Nl are the number of distinct POS tags and arc labels. Lastly let the number of chosen elements from each set of features be denoted as nword ntag and nlabel respectively. Feature Selection Example As an example consider the following choices for Sword Stag and Slabel. 1. Sword The top 3 words on the stack and buffer s1 s2 s3 b1 b2 b3. The ﬁrst and second leftmost rightmost children of the top two words on the stack lc1 si rc1 si lc2 si rc2 si i 1 2. The leftmost of leftmost rightmost of rightmost children of the top two words on the stack lc1 lc1 si rc1 rc1 si i 1 2. In total Sword contains nw 18 elements. 2. Stag The corresponding POS tags for Stag nt 18 . 3. Slabel The corresponding arc labels of words excluding those 6 words on the stack buffer nl 12 . Note that we use a special Null token for non existent elements when the stack and buffer are empty or dependents have not been assigned yet. For a given sentence example we select the words POS tags and arc labels given the schematic deﬁned above extract their corresponding dense feature representations produced from the embedding matrices Ew Et and El and concatenate these vectors into our inputs xw xt xl . At training time we backpropagate into the dense vector representations as well as the parameters at later layers. Feedforward Neural Network Model The network contains an input layer xw xt xl a hidden layer and a ﬁnal softmax layer with a cross entropy loss function. We can either deﬁne a single weight matrix in the hidden layer to operate on a concatenation of xw xt xl or we can use three weight matrices Ww 1 Wt 1 Wl 1 one for each input type as shown in Figure 3. We then apply a non linear function and use one more afﬁne layer W2 so that there are an equivalent number of softmax probabilities to the number of possible transitions the output dimension . Input layer xw xt xl Hidden layer h Ww 1 xw Wt 1xt Wl 1xl b1 3 Softmax layer p softmax W2h words POS tags arc labels ROOT has_VBZ He_PRP nsubj has_VBZ good_JJ control_NN ._. Stack Buffer Conﬁguration Figure 3 The neural network archi tecture for greedy transition based dependency parsing. Note that in Figure 3 f x x3 is the non linear function used. For a more complete explanation of a greedy transition based neural dependency parser refer to A Fast and Accurate Dependency Parser using Neural Networks under Further Reading. Further reading Danqi Chen and Christopher D. Manning. A Fast and Accurate Dependency Parser using Neural Networks. EMNLP. 2014. Kuebler Sandra Ryan McDonald and Joakim Nivre. Depen dency parsing. Synthesis Lectures on Human Language Technolo gies 1.1 2009 1 127.