Learning Dense Representations of Phrases at Scale Jinhyuk Lee1 2 Mujeen Sung1 Jaewoo Kang1 Danqi Chen2 Korea University1 Princeton University2 jinhyuk_lee mujeensung kangj @korea.ac.kr danqic@cs.princeton.edu Abstract Open domain question answering can be refor mulated as a phrase retrieval problem without the need for processing documents on demand during inference Seo et al. 2019 . However current phrase retrieval models heavily depend on sparse representations and still underper form retriever reader approaches. In this work we show for the ﬁrst time that we can learn dense representations of phrases alone that achieve much stronger performance in open domain QA. We present an effective method to learn phrase representations from the super vision of reading comprehension tasks cou pled with novel negative sampling methods. We also propose a query side ﬁne tuning strat egy which can support transfer learning and reduce the discrepancy between training and inference. On ﬁve popular open domain QA datasets our model DensePhrases improves over previous phrase retrieval models by 15 25 absolute accuracy and matches the perfor mance of state of the art retriever reader mod els. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally we directly use our pre indexed dense phrase representations for two slot ﬁlling tasks show ing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.1 1 Introduction Open domain question answering QA aims to provide answers to natural language questions us ing a large text corpus Voorhees et al. 1999 Fer rucci et al. 2010 Chen and Yih 2020 . While a dominating approach is a two stage retriever reader approach Chen et al. 2017 Lee et al. 2019 Guu et al. 2020 Karpukhin et al. 2020 we focus on Work partly done while visiting Princeton University. 1Our code is available at https github.com princeton nlp DensePhrases. a recent new paradigm solely based on phrase re trieval Seo et al. 2019 Lee et al. 2020 . Phrase retrieval highlights the use of phrase representa tions and ﬁnds answers purely based on the similar ity search in the vector space of phrases.2 Without relying on an expensive reader model for process ing text passages it has demonstrated great runtime efﬁciency at inference time. Despite great promise it remains a formidable challenge to build vector representations for ev ery single phrase in a large corpus. Since phrase representations are decomposed from question rep resentations they are inherently less expressive than cross attention models Devlin et al. 2019 . Moreover the approach requires retrieving answers correctly out of billions of phrases e.g. 6 1010 phrases in English Wikipedia making the scale of the learning problem difﬁcult. Consequently ex isting approaches heavily rely on sparse represen tations for locating relevant documents and para graphs while still falling behind retriever reader models Seo et al. 2019 Lee et al. 2020 . In this work we investigate whether we can build fully dense phrase representations at scale for open domain QA. First we aim to learn strong phrase representations from the supervision of reading comprehension tasks. We propose to use data aug mentation and knowledge distillation to learn better phrase representations within a single passage. We then adopt negative sampling strategies such as in batch negatives Henderson et al. 2017 Karpukhin et al. 2020 to better discriminate the phrases at a larger scale. Here we present a novel method called pre batch negatives which leverages preced ing mini batches as negative examples to compen sate the need of large batch training. Lastly we present a query side ﬁne tuning strategy that dras 2Following previous work Seo et al. 2018 phrase de notes any contiguous segment of text up to L words including single words which is not necessarily a linguistic phrase. arXiv 2012.12624v3 cs.CL 2 Jun 2021Category Model Sparse Storage Q sec NQ SQuAD GB GPU CPU Acc Acc Retriever Reader DrQA Chen et al. 2017 26 1.8 0.6 29.8 BERTSerini Yang et al. 2019 21 2.0 0.4 38.6 ORQA Lee et al. 2019 18 8.6 1.2 33.3 20.2 REALMNews Guu et al. 2020 18 8.4 1.2 40.4 DPR multi Karpukhin et al. 2020 76 0.9 0.04 41.5 24.1 Phrase Retrieval DenSPI Seo et al. 2019 1 200 2.9 2.4 8.1 36.2 DenSPI Sparc Lee et al. 2020 1 547 2.1 1.7 14.5 40.7 DensePhrases Ours 320 20.6 13.6 40.9 38.0 Table 1 Retriever reader and phrase retrieval approaches for open domain QA. The retriever reader approach retrieves a small number of relevant documents or passages from which the answers are extracted. The phrase retrieval approach retrieves an answer out of billions of phrase representations pre indexed from the entire corpus. Appendix B provides detailed benchmark speciﬁcation. The accuracy is measured on the test sets in the open domain setting. NQ Natural Questions. tically improves phrase retrieval performance and allows for transfer learning to new domains with out re building billions of phrase representations. As a result all these improvements lead to a much stronger phrase retrieval model without the use of any sparse representations Table 1 . We evaluate our model DensePhrases on ﬁve standard open domain QA datasets and achieve much bet ter accuracies than previous phrase retrieval mod els Seo et al. 2019 Lee et al. 2020 with 15 25 absolute improvement on most datasets. Our model also matches the performance of state of the art retriever reader models Guu et al. 2020 Karpukhin et al. 2020 . Due to the removal of sparse representations and careful design choices we further reduce the storage footprint for the full English Wikipedia from 1.5TB to 320GB as well as drastically improve the throughput. Finally we envision that DensePhrases acts as a neural interface for retrieving phrase level knowl edge from a large text corpus. To showcase this possibility we demonstrate that we can directly use DensePhrases for fact extraction without re building the phrase storage. With only ﬁne tuning the question encoder on a small number of subject relation object triples we achieve state of the art performance on two slot ﬁlling tasks Petroni et al. 2021 using less than 5 of the training data. 2 Background We ﬁrst formulate the task of open domain ques tion answering for a set of K documents D d1 . . . dK . We follow the recent work Chen et al. 2017 Lee et al. 2019 and treat all of English Wikipedia as D hence K 5 106. However most approaches including ours are generic and could be applied to other collections of documents. The task aims to provide an answer ˆ a for the in put question q based on D. In this work we focus on the extractive QA setting where each answer is a segment of text or a phrase that can be found in D. Denote the set of phrases in D as S D and each phrase sk S D consists of contiguous words wstart k . . . wend k in its document ddoc k . In practice we consider all the phrases up to L 20 words in D and S D comprises a large number of 6 1010 phrases. An extractive QA system returns a phrase ˆ s argmaxs S D f s D q where f is a scoring function. The system ﬁnally maps ˆ s to an answer string ˆ a TEXT ˆ s ˆ a and the evalua tion is typically done by comparing the predicted answer ˆ a with a gold answer a . Although we focus on the extractive QA setting recent works propose to use a generative model as the reader Lewis et al. 2020 Izacard and Grave 2021 or learn a closed book QA model Roberts et al. 2020 which directly predicts answers with out using an external knowledge source. The ex tractive setting provides two advantages ﬁrst the model directly locates the source of the answer which is more interpretable and second phrase level knowledge retrieval can be uniquely adapted to other NLP tasks as we show in 7.3. Retriever reader. A dominating paradigm in open domain QA is the retriever reader ap proach Chen et al. 2017 Lee et al. 2019 Karpukhin et al. 2020 which leverages a ﬁrst stage document retriever fretr and only reads top K K documents with a reader model fread. The scoring function f s D q is decomposed asf s D q fretr dj1 . . . djK D q fread s dj1 . . . djK q 1 where j1 . . . jK 1 . . . K and if s S dj1 . . . djK the score will be 0. It can eas ily adapt to passages and sentences Yang et al. 2019 Wang et al. 2019 . However this approach suffers from error propagation when incorrect docu ments are retrieved and can be slow as it usually re quires running an expensive reader model on every retrieved document or passage at inference time. Phrase retrieval. Seo et al. 2019 introduce the phrase retrieval approach that encodes phrase and question representations independently and per forms similarity search over the phrase representa tions to ﬁnd an answer. Their scoring function f is computed as follows f s D q Es s D Eq q 2 where Es and Eq denote the phrase encoder and the question encoder respectively. As Es and Eq representations are decomposable it can support maximum inner product search MIPS and improve the efﬁciency of open domain QA models. Previous approaches Seo et al. 2019 Lee et al. 2020 leverage both dense and sparse vectors for phrase and question representations by taking their concatenation Es s D Esparse s D Edense s D .3 However since the sparse vectors are difﬁcult to parallelize with dense vectors their method essentially conducts sparse and dense vector search separately. The goal of this work is to only use dense representations i.e. Es s D Edense s D which can model f s D q solely with MIPS as well as close the gap in performance. 3 DensePhrases 3.1 Overview We introduce DensePhrases a phrase retrieval model that is built on fully dense representations. Our goal is to learn a phrase encoder as well as a question encoder so we can pre index all the pos sible phrases in D and efﬁciently retrieve phrases for any question through MIPS at testing time. We outline our approach as follows 3Seo et al. 2019 use sparse representations of both para graphs and documents and Lee et al. 2020 use contextualized sparse representations conditioned on the phrase. We ﬁrst learn a high quality phrase encoder and an initial question encoder from the supervision of reading comprehension tasks 4.1 as well as incorporating effective nega tive sampling to better discriminate phrases at scale 4.2 4.3 . Then we ﬁx the phrase encoder and encode all the phrases s S D and store the phrase indexing ofﬂine to enable efﬁcient search 5 . Finally we introduce an additional strategy called query side ﬁne tuning 6 by further updating the question encoder.4 We ﬁnd this step to be very effective as it can reduce the discrepancy between training the ﬁrst step and inference as well as support transfer learning to new domains. Before we present the approach in detail we ﬁrst describe our base architecture below. 3.2 Base Architecture Our base architecture consists of a phrase encoder Es and a question encoder Eq. Given a passage p w1 . . . wm we denote all the phrases up to L tokens as S p . Each phrase sk has start and end in dicies start k and end k and the gold phrase is s S p . Following previous work on phrase or span representations Lee et al. 2017 Seo et al. 2018 we ﬁrst apply a pre trained language model Mp to obtain contextualized word representations for each passage token h1 . . . hm Rd. Then we can represent each phrase sk S p as the con catenation of corresponding start and end vectors Es sk p hstart k hend k R2d. 3 A great advantage of this representation is that we eventually only need to index and store all the word vectors we use W D to denote all the words in D instead of all the phrases S D which is at least one magnitude order smaller. Similarly we need to learn a question encoder Eq that maps a question q w1 . . . wn to a vector of the same dimension as Es . Since the start and end representations of phrases are pro duced by the same language model we use an other two different pre trained encoders Mq start and Mq end to differentiate the start and end po sitions. We apply Mq start and Mq end on q sep arately and obtain representations qstart and qend 4In this paper we use the term question and query inter changeably as our question encoder can be naturally extended to unnatural queries.Ǜ ǜ . ǆ ǅ . ǆ . . 4 . ƺƺƺ Ǜ ǜ . . ǅ . ǂ ƾ ǅ 0 . ƾ . ƾ 2 2 . 2 Ǜ ǜ ǅ . Ǜ ǜ .0 4 . 1 1 Ǚ ǚ Ǒ .. 2ǟ 1 . Ǚ ǚ 0 4Ǒ. Ɵ Ǒ 0 ƥ ƾ Ǜ ǜ 0 . ǅ . 1 . Ǚ Ǒ ǟ Ǒ ǚ Ǜ ǜ Ǜ ǜ Ǜ ǜ . 0 . Ǜ ǜ ƾ . ǑƧ Ǒƨ Figure 1 An overview of DensePhrases. a We learn dense phrase representations in a single passage 4.1 along with in batch and pre batch negatives 4.2 4.3 . b With the top k retrieved phrase representations from the entire text corpus 5 we further perform query side ﬁne tuning to optimize the question encoder 6 . During inference our model simply returns the top 1 prediction. taken from the CLS token representations re spectively. Finally Eq simply takes their con catenation Eq q qstart qend R2d. 4 Note that we use pre trained language models to initialize Mp Mq start and Mq end and they are ﬁne tuned with the objectives that we will deﬁne later. In our pilot experiments we found that Span BERT Joshi et al. 2020 leads to superior perfor mance compared to BERT Devlin et al. 2019 . SpanBERT is designed to predict the information in the entire span from its two endpoints therefore it is well suited for our phrase representations. In our ﬁnal model we use SpanBERT base cased as our base LMs for Es and Eq and hence d 768.5 See Table 5 for an ablation study. 4 Learning Phrase Representations In this section we start by learning dense phrase representations from the supervision of reading comprehension tasks i.e. a single passage p con tains an answer a to a question q. Our goal is to learn strong dense representations of phrases for s S p which can be retrieved by a dense rep resentation of the question and serve as a direct 5Our base model is largely inspired by DenSPI Seo et al. 2019 although we deviate from theirs as follows. 1 We remove coherency scalars and don t split any vectors. 2 DenSPI uses a shared encoder for phrases and questions while we use 3 separate language models initialized from the same pre trained model. 3 We use SpanBERT instead of BERT. answer 4.1 . Then we introduce two different negative sampling methods 4.2 4.3 which en courage the phrase representations to be better dis criminated at the full Wikipedia scale. See Figure 1 for an overview of DensePhrases. 4.1 Single passage Training To learn phrase representations in a single passage along with question representations we ﬁrst max imize the log likelihood of the start and end posi tions of the gold phrase s where TEXT s a . The training loss for predicting the start position of a phrase given a question is computed as zstart 1 . . . zstart m h 1 qstart . . . h mqstart P start softmax zstart 1 . . . zstart m Lstart log P start start s . 5 We can deﬁne Lend in a similar way and the ﬁnal loss for the single passage training is Lsingle Lstart Lend 2 . 6 This essentially learns reading comprehension with out any cross attention between the passage and the question tokens which fully decomposes phrase and question representations. Data augmentation Since the contextualized word representations h1 . . . hm are encoded in a query agnostic way they are always inferior toquery dependent representations in cross attention models Devlin et al. 2019 where passages are fed along with the questions concatenated by a spe cial token such as SEP . We hypothesize that one key reason for the performance gap is that reading comprehension datasets only provide a few anno tated questions in each passage compared to the set of possible answer phrases. Learning from this su pervision is not easy to differentiate similar phrases in one passage e.g. s Charles Prince of Wales and another s Prince George for a question q Who is next in line to be the monarch of England . Following this intuition we propose to use a sim ple model to generate additional questions for data augmentation based on a T5 large model Raf fel et al. 2020 . To train the question genera tion model we feed a passage p with the gold answer s highlighted by inserting surrounding special tags. Then the model is trained to max imize the log likelihood of the question words of q. After training we extract all the named enti ties in each training passage as candidate answers and feed the passage p with each candidate an swer to generate questions. We keep the question answer pairs only when a cross attention reading comprehension model6 makes a correct prediction on the generated pair. The remaining generated QA pairs q1 s1 q2 s2 . . . qr sr are directly augmented to the original training set. Distillation We also propose improving the phrase representations by distilling knowledge from a cross attention model Hinton et al. 2015 . We minimize the Kullback Leibler divergence be tween the probability distribution from our phrase encoder and that from a standard SpanBERT base QA model. The loss is computed as follows Ldistill KL P start P start c KL P end P end c 2 7 where P start and P end is deﬁned in Eq. 5 and P start c and P end c denote the probability distributions used to predict the start and end positions of an swers in the cross attention model. 4.2 In batch Negatives Eventually we need to build phrase representations for billions of phrases. Therefore a bigger chal lenge is to incorporate more phrases as negatives so the representations can be better discriminated 6SpanBERT large 88.2 EM on SQuAD. Positive a In batch Negatives B 1 b Pre batch Negatives B C Detached in recent C batches gstart i Negative gstart 1 gstart 2 gstart 3 gstart 4 qstart 1 qstart 2 qstart 3 qstart 4 qstart 1 qstart 2 qstart 3 qstart 4 Figure 2 Two types of negative samples for the ﬁrst batch item qstart 1 in a mini batch of size B 4 and C 3. Note that the negative samples for the end representations qend i are obtained in a similar manner. See 4.2 and 4.3 for more details. at a larger scale. While Seo et al. 2019 simply sample two negative passages based on question similarity we use in batch negatives for our dense phrase representations which has been shown to be effective in learning dense passage representations before Karpukhin et al. 2020 . As shown in Figure 2 a for the i th exam ple in a mini batch of size B we denote the hidden representations of the gold start and end positions hstart s and hend s as gstart i and gend i as well as the question representation as qstart i qend i . Let Gstart Gend Qstart Qend be the B d matrices and each row corresponds to gstart i g end i qstart i qend i respectively. Basically we can treat all the gold phrases from other pas sages in the same mini batch as negative exam ples. We compute Sstart QstartGstart and Send QendGend and the i th row of Sstart and Send return B scores each including a positive score and B 1 negative scores sstart 1 . . . sstart B and send 1 . . . send B . Similar to Eq. 5 we can compute the loss func tion for the i th example as P start_ib i softmax sstart 1 . . . sstart B P end_ib i softmax send 1 . . . send B Lneg log P start_ib i log P end_ib i 2 8 We also attempted using non gold phrases from other passages as negatives but did not ﬁnd a mean ingful improvement. 4.3 Pre batch Negatives The in batch negatives usually beneﬁt from a large batch size Karpukhin et al. 2020 . However it is challenging to further increase batch sizes as they are bounded by the size of GPU memory. Next we propose a novel negative sampling methodcalled pre batch negatives which can effectively utilize the representations from the preceding C mini batches Figure 2 b . In each iteration we maintain a FIFO queue of C mini batches to cache phrase representations Gstart and Gend. The cached phrase representations are then used as negative samples for the next iteration providing B C additional negative samples in total.7 These pre batch negatives are used together with in batch negatives and the training loss is the same as Eq. 8 except that the gradients are not back propagated to the cached pre batch negatives. After warming up the model with in batch negatives we simply shift from in batch negatives B 1 nega tives to in batch and pre batch negatives hence a total number of B C B 1 negatives . For sim plicity we use Lneg to denote the loss for both in batch negatives and pre batch negatives. Since we do not retain the computational graph for pre batch negatives the memory consumption of pre batch negatives is much more manageable while allowing an increase in the number of negative samples. 4.4 Training Objective Finally we optimize all the three losses together on both annotated reading comprehension examples and generated questions from 4.1 L λ1Lsingle λ2Ldistill λ3Lneg 9 where λ1 λ2 λ3 determine the importance of each loss term. We found that λ1 1 λ2 2 and λ3 4 works well in practice. See Table 5 and Table 6 for an ablation study of different components. 5 Indexing and Search Indexing After training the phrase encoder Es we need to encode all the phrases S D in the en tire English Wikipedia D and store an index of the phrase dump. We segment each document di D into a set of natural paragraphs from which we obtain token representations for each paragraph using Es . Then we build a phrase dump H h1 . . . h W D R W D d by stacking the token representations from all the para graphs in D. Note that this process is computation ally expensive and takes about hundreds of GPU hours with a large disk footprint. To reduce the 7This approach is inspired by the momentum contrast idea proposed in unsupervised visual representation learning He et al. 2020 . Contrary to their approach we have separate encoders for phrases and questions and back propagate to both during training without a momentum update. size of phrase dump we follow and modify several techniques introduced in Seo et al. 2019 see Ap pendix E for details . After indexing we can use two rows i and j of H to represent a dense phrase representation hi hj . We use faiss Johnson et al. 2017 for building a MIPS index of H.8 Search For a given question q we can ﬁnd the answer ˆ s as follows ˆ s argmax s i j Es s i j D Eq q argmax s i j Hqstart i Hqend j 10 where s i j denotes a phrase with start and end indices as i and j in the index H. We can com pute the argmax of Hqstart and Hqend efﬁciently by performing MIPS over H with qstart and qend. In practice we search for the top k start and top k end positions separately and perform a constrained search over their end and start positions respec tively such that 1 i j i L W D . 6 Query side Fine tuning So far we have created a phrase dump H that sup ports efﬁcient MIPS search. In this section we pro pose a novel method called query side ﬁne tuning by only updating the question encoder Eq to cor rectly retrieve a desired answer a for a question q given H. Formally speaking we optimize the marginal log likelihood of the gold answer a for a question q which resembles the weakly supervised QA setting in previous work Lee et al. 2019 Min et al. 2019 . For every question q we retrieve top k phrases and minimize the objective Lquery log P s S q TEXT s a exp f s D q P s S q exp f s D q 11 where f s D q is the score of the phrase s Eq. 2 and S q denotes the top k phrases for q Eq. 10 . In practice we use k 100 for all the experiments. There are several advantages for doing this 1 we ﬁnd that query side ﬁne tuning can reduce the discrepancy between training and inference and hence improve the ﬁnal performance substantially 8 . Even with effective negative sampling the model only sees a small portion of passages com pared to the full scale of D and this training objec tive can effectively ﬁll in the gap. 2 This train ing strategy allows for transfer learning to unseen 8We use IVFSQ4 with 1M clusters and set n probe to 256.domains without rebuilding the entire phrase in dex. More speciﬁcally the model is able to quickly adapt to new QA tasks e.g. WebQuestions when the phrase dump is built using SQuAD or Natural Questions. We also ﬁnd that this can transfers to non QA tasks when the query is written in a dif ferent format. In 7.3 we show the possibility of directly using DensePhrases for slot ﬁlling tasks by using a query such as Michael Jackson is a singer of x . In this regard we can view our model as a dense knowledge base that can be accessed by many different types of queries and it is able to return phrase level knowledge efﬁciently. 7 Experiments 7.1 Setup Datasets. We use two reading comprehension datasets SQuAD Rajpurkar et al. 2016 and Nat ural Questions NQ Kwiatkowski et al. 2019 to learn phrase representations in which a single gold passage is provided for each question. For the open domain QA experiments we evaluate our approach on ﬁve popular open domain QA datasets Natu ral Questions WebQuestions WQ Berant et al. 2013 CuratedTREC TREC Baudiš and Šediv y 2015 TriviaQA TQA Joshi et al. 2017 and SQuAD. Note that we only use SQuAD and or NQ to build the phrase index and perform query side ﬁne tuning 6 for other datasets. We also evaluate our model on two slot ﬁlling tasks to show how to adapt our DensePhrases for other knowledge intensive NLP tasks. We focus on using two slot ﬁlling datasets from the KILT benchmark Petroni et al. 2021 T REx Elsahar et al. 2018 and zero shot relation extraction Levy et al. 2017 . Each query is provided in the form of subject entity SEP relation and the answer is the object entity. Appendix C provides the statistics of all the datasets. Implementation details. We denote the training datasets used for reading comprehension Eq. 9 as Cphrase. For open domain QA we train two ver sions of phrase encoders each of which are trained on Cphrase SQuAD and NQ SQuAD re spectively. We build the phrase dump H for the 2018 12 20 Wikipedia snapshot and perform query side ﬁne tuning on each dataset using Eq. 11 . For slot ﬁlling we use the same phrase dump for open domain QA Cphrase NQ SQuAD and perform query side ﬁne tuning on randomly sampled 5K Model SQuAD NQ Long EM F1 EM F1 Query Dependent BERT base 80.8 88.5 69.9 78.2 SpanBERT base 85.7 92.2 73.2 81.0 Query Agnostic DilBERT Siblini et al. 2020 63.0 72.0 DeFormer Cao et al. 2020 72.1 DenSPI 73.6 81.7 68.2 76.1 DenSPI Sparc 76.4 84.8 DensePhrases ours 78.3 86.3 71.9 79.6 Table 2 Reading comprehension results evaluated on the development sets of SQuAD and Natural Ques tions. Underlined numbers are estimated from the ﬁg ures from the original papers. BERT large model. or 10K training examples to see how rapidly our model adapts to the new query types. See Ap pendix D for details on the hyperparameters and Appendix A for an analysis of computational cost. 7.2 Experiments Question Answering Reading comprehension. In order to show the effectiveness of our phrase representations we ﬁrst evaluate our model in the reading comprehension setting for SQuAD and NQ and report its perfor mance with other query agnostic models Eq. 9 without query side ﬁne tuning . This problem was originally formulated by Seo et al. 2018 as the phrase indexed question answering PIQA task. Compared to previous query agnostic models our model achieves the best performance of 78.3 EM on SQuAD by improving the previous phrase retrieval model DenSPI by 4.7 Table 2 . Al though it is still behind cross attention models the gap has been greatly reduced and serves as a strong starting point for the open domain QA model. Open domain QA. Experimental results on open domain QA are summarized in Table 3. With out any sparse representations DensePhrases out performs previous phrase retrieval models by a large margin and achieves a 15 25 absolute improvement on all datasets except SQuAD. Train ing the model of Lee et al. 2020 on Cphrase NQ SQuAD only increases the result from 14.5 to 16.5 on NQ demonstrating that it does not sufﬁce to simply add more datasets for train ing phrase representations. Our performance is also competitive with recent retriever reader mod els Karpukhin et al. 2020 while running much faster during inference Table 1 .Model NQ WQ TREC TQA SQuAD Retriever reader Cretr Pre Training DrQA Chen et al. 2017 20.7 25.4 29.8 BERT BM25 Lee et al. 2019 26.5 17.7 21.3 47.1 33.2 ORQA Lee et al. 2019 Wiki. 33.3 36.4 30.1 45.0 20.2 REALMNews Guu et al. 2020 Wiki. CC News 40.4 40.7 42.9 DPR multi Karpukhin et al. 2020 NQ WQ TREC TQA 41.5 42.4 49.4 56.8 24.1 Phrase retrieval Cphrase Training DenSPI Seo et al. 2019 SQuAD 8.1 11.1 31.6 30.7 36.2 DenSPI Sparc Lee et al. 2020 SQuAD 14.5 17.3 35.7 34.4 40.7 DenSPI Sparc Lee et al. 2020 NQ SQuAD 16.5 DensePhrases ours SQuAD 31.2 36.3 50.3 53.6 39.4 DensePhrases ours NQ SQuAD 40.9 37.5 51.0 50.7 38.0 Table 3 Open domain QA results. We report exact match EM on the test sets. We also show the additional training or pre training datasets for learning the retriever models Cretr and creating the phrase dump Cphrase . no supervision using target training data zero shot . unlabeled data used for extra pre training. Model T REx ZsRE Acc F1 Acc F1 DPR BERT 4.47 27.09 DPR BART 11.12 11.41 18.91 20.32 RAG 23.12 23.94 36.83 39.91 DensePhrases5K 25.32 29.76 40.39 45.89 DensePhrases10K 27.84 32.34 41.34 46.79 Table 4 Slot ﬁlling results on the test sets of T REx and Zero shot RE ZsRE in the KILT benchmark. We report KILT AC and KILT F1 denoted as Acc and F1 in the table which consider both span level accuracy and correct retrieval of evidence documents. 7.3 Experiments Slot Filling Table 4 summarizes the results on the two slot ﬁll ing datasets along with the baseline scores pro vided by Petroni et al. 2021 . The only extractive baseline is DPR BERT which performs poorly in zero shot relation extraction. On the other hand our model achieves competitive performance on all datasets and achieves state of the art performance on two datasets using only 5K training examples. 8 Analysis Ablation of phrase representations. Table 5 shows the ablation result of our model on SQuAD. Upon our choice of architecture augmenting train ing set with generated questions QG and performing distillation from cross attention mod els Distill improve performance up to EM 78.3. We attempted adding the generated questions to the training of the SpanBERT QA model but ﬁnd a 0.3 improvement which validates that data sparsity is a bottleneck for query agnostic models. Model M Share Split QG Distill EM DenSPI Bb. 70.2 Sb. 68.5 Bl. 73.6 Dense Bb. 70.2 Phrases Bb. 71.9 Sb. 73.2 Sb. 76.3 Sb. 78.3 Table 5 Ablation of DensePhrases on the development set of SQuAD. Bb BERT base Sb SpanBERT base Bl BERT large. Share whether question and phrase encoders are shared or not. Split whether the full hidden vectors are kept or split into start and end vec tors. QG question generation 4.1 . Distill distilla tion Eq. 7 . DenSPI Seo et al. 2019 also included a coherency scalar and see their paper for more details. Effect of batch negatives. We further evaluate the effectiveness of various negative sampling methods introduced in 4.2 and 4.3. Since it is computationally expensive to test each setting at the full Wikipedia scale we use a smaller text cor pus Dsmall of all the gold passages in the develop ment sets of Natural Questions for the ablation study. Empirically we ﬁnd that results are gener ally well correlated when we gradually increase the size of D . As shown in Table 6 both in batch and pre batch negatives bring substantial improve ments. While using a larger batch size B 84 is beneﬁcial for in batch negatives the number of preceding batches in pre batch negatives is optimal when C 2. Surprisingly the pre batch negatives also improve the performance when D p .Type B C D p D Dsmall None 48 70.4 35.3 In batch 48 70.5 52.4 84 70.3 54.2 Pre batch 84 1 71.6 59.8 84 2 71.9 60.4 84 4 71.2 59.8 Table 6 Effect of in batch negatives and pre batch neg atives on the development set of Natural Questions. B batch size. C number of preceding mini batches used in pre batch negatives. Dsmall all the gold passages in the development set of NQ. p single passage. Effect of query side ﬁne tuning. We summa rize the effect of query side ﬁne tuning in Table 7. For the datasets that were not used for training the phrase encoders TQA WQ TREC we observe a 15 to 20 improvement after query side ﬁne tuning. Even for the datasets that have been used NQ SQuAD it leads to signiﬁcant improvements e.g. 32.6 40.9 on NQ for Cphrase NQ and it clearly demonstrates it can effectively reduce the discrepancy between training and inference. 9 Related Work Learning effective dense representations of words is a long standing goal in NLP Bengio et al. 2003 Collobert et al. 2011 Mikolov et al. 2013 Peters et al. 2018 Devlin et al. 2019 . Beyond words dense representations of many different granular ities of text such as sentences Le and Mikolov 2014 Kiros et al. 2015 or documents Yih et al. 2011 have been explored. While dense phrase rep resentations have been also studied for statistical machine translation Cho et al. 2014 or syntactic parsing Socher et al. 2010 our work focuses on learning dense phrase representations for QA and any other knowledge intensive tasks where phrases can be easily retrieved by performing MIPS. This type of dense retrieval has been also stud ied for sentence and passage retrieval Humeau et al. 2019 Karpukhin et al. 2020 see Lin et al. 2020 for recent advances in dense retrieval . While DensePhrases is explicitly designed to retrieve phrases that can be used as an answer to given queries retrieving phrases also naturally entails re trieving larger units of text provided the datastore maintains the mapping between each phrase and the sentence and passage in which it occurs. QS NQ WQ TREC TQA SQuAD Cphrase SQuAD 12.3 11.8 36.9 34.6 35.5 31.2 36.3 50.3 53.6 39.4 Cphrase NQ 32.6 21.1 32.3 32.4 20.7 40.9 37.1 49.7 49.2 25.7 Cphrase NQ SQuAD 28.9 18.9 34.9 31.9 33.2 40.9 37.5 51.0 50.7 38.0 Table 7 Effect of query side ﬁne tuning in DensePhrases on each test set. We report EM of each model before QS and after QS the query side ﬁne tuning. 10 Conclusion In this study we show that we can learn dense repre sentations of phrases at the Wikipedia scale which are readily retrievable for open domain QA and other knowledge intensive NLP tasks. We learn both phrase and question encoders from the supervi sion of reading comprehension tasks and introduce two batch negative techniques to better discrimi nate phrases at scale. We also introduce query side ﬁne tuning that adapts our model to different types of queries. We achieve strong performance on ﬁve popular open domain QA datasets while reducing the storage footprint and improving latency signif icantly. We also achieve strong performance on two slot ﬁlling datasets using only a small number of training examples showing the possibility of utilizing our DensePhrases as a knowledge base. Acknowledgments We thank Sewon Min Hyunjae Kim Gyuwan Kim Jungsoo Park Zexuan Zhong Dan Fried man Chris Sciavolino for providing valuable com ments and feedback. This research was supported by a grant of the Korea Health Technology R D Project through the Korea Health Industry Develop ment Institute KHIDI funded by the Ministry of Health Welfare Republic of Korea grant num ber HR20C0021 and National Research Foun dation of Korea NRF 2020R1A2C3010638 . It was also partly supported by the James Mi 91 Re search Innovation Fund for Data Science and an Amazon Research Award.Ethical Considerations Our work builds on standard reading comprehen sion datasets such as SQuAD to build phrase rep resentations. SQuAD in particular is created from a small number of Wikipedia articles sampled from top 10 000 most popular articles measured by PageRanks hence some of our models trained only on SQuAD could be easily biased towards the small number of topics that SQuAD contains. We hope that excluding such datasets during training or inventing an alternative pre training procedure for learning phrase representations could mitigate this problem. Although most of our efforts have been made to reduce the computational complexity of previous phrase retrieval models further detailed in Appendices A and E leveraging our phrase re trieval model as a knowledge base will inevitably increase the minimum requirement for the addi tional experiments. We plan to apply vector quanti zation techniques to reduce the additional cost of using our model as a KB. References Akari Asai Kazuma Hashimoto Hannaneh Hajishirzi Richard Socher and Caiming Xiong. 2020. Learn ing to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations ICLR . Petr Baudiš and Jan Šediv y. 2015. Modeling of the question answering task in the YodaQA system. In International Conference of the Cross Language Evaluation Forum for European Languages CLEF . Yoshua Bengio Réjean Ducharme Pascal Vincent and Christian Jauvin. 2003. A neural probabilistic lan guage model. The Journal of Machine Learning Re search JMLR . Jonathan Berant Andrew Chou Roy Frostig and Percy Liang. 2013. Semantic parsing on Freebase from question answer pairs. In Empirical Methods in Nat ural Language Processing EMNLP . Qingqing Cao Harsh Trivedi Aruna Balasubramanian and Niranjan Balasubramanian. 2020. Deformer Decomposing pre trained Transformers for faster question answering. In Association for Computa tional Linguistics ACL . Danqi Chen Adam Fisch Jason Weston and Antoine Bordes. 2017. Reading Wikipedia to answer open domain questions. In Association for Computa tional Linguistics ACL . Danqi Chen and Wen tau Yih. 2020. Open domain question answering. In Association for Computa tional Linguistics ACL . Kyunghyun Cho Bart Van Merriënboer Caglar Gul cehre Dzmitry Bahdanau Fethi Bougares Holger Schwenk and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder decoder for statistical machine translation. In Empirical Methods in Natural Language Processing EMNLP . Ronan Collobert Jason Weston Léon Bottou Michael Karlen Koray Kavukcuoglu and Pavel Kuksa. 2011. Natural language processing almost from scratch. JMLR. Jacob Devlin Ming Wei Chang Kenton Lee and Kristina Toutanova. 2019. BERT Pre training of deep bidirectional Transformers for language under standing. In North American Chapter of the Associ ation for Computational Linguistics NAACL . Hady Elsahar Pavlos Vougiouklis Arslen Remaci Christophe Gravier Jonathon Hare Frederique Laforest and Elena Simperl. 2018. T REx A large scale alignment of natural language with knowledge base triples. In International Conference on Lan guage Resources and Evaluation LREC . David Ferrucci Eric Brown Jennifer Chu Carroll James Fan David Gondek Aditya A Kalyanpur Adam Lally J William Murdock Eric Nyberg John Prager et al. 2010. Building Watson An overview of the deepqa project. AI magazine 31 3 . Kelvin Guu Kenton Lee Zora Tung Panupong Pa supat and Ming Wei Chang. 2020. REALM Retrieval augmented language model pre training. In International Conference on Machine Learning ICML . Kaiming He Haoqi Fan Yuxin Wu Saining Xie and Ross Girshick. 2020. Momentum contrast for un supervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recog nition CVPR . Matthew Henderson Rami Al Rfou Brian Strope Yun Hsuan Sung László Lukács Ruiqi Guo Sanjiv Ku mar Balint Miklos and Ray Kurzweil. 2017. Efﬁ cient natural language response suggestion for smart reply. arXiv preprint arXiv 1705.00652. Geoffrey Hinton Oriol Vinyals and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv 1503.02531. Samuel Humeau Kurt Shuster Marie Anne Lachaux and Jason Weston. 2019. Poly encoders Architec tures and pre training strategies for fast and accurate multi sentence scoring. In International Conference on Learning Representations ICLR . Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In European Chap ter of the Association for Computational Linguistics EACL .Jeff Johnson Matthijs Douze and Hervé Jégou. 2017. Billion scale similarity search with GPUs. arXiv preprint arXiv 1702.08734. Mandar Joshi Danqi Chen Yinhan Liu Daniel S Weld Luke Zettlemoyer and Omer Levy. 2020. Span BERT Improving pre training by representing and predicting spans. Transactions of the Association of Computational Linguistics TACL . Mandar Joshi Eunsol Choi Daniel S Weld and Luke Zettlemoyer. 2017. TriviaQA A large scale dis tantly supervised challenge dataset for reading com prehension. In Association for Computational Lin guistics ACL . Vladimir Karpukhin Barlas O guz Sewon Min Patrick Lewis Ledell Wu Sergey Edunov Danqi Chen and Wen tau Yih. 2020. Dense passage retrieval for open domain question answering. In Empirical Methods in Natural Language Processing EMNLP . Diederik P Kingma and Jimmy Ba. 2015. Adam A method for stochastic optimization. In International Conference on Learning Representations ICLR . Ryan Kiros Yukun Zhu Russ R Salakhutdinov Richard Zemel Raquel Urtasun Antonio Torralba and Sanja Fidler. 2015. Skip thought vectors. Ad vances in Neural Information Processing Systems NIPS . Tom Kwiatkowski Jennimaria Palomaki Olivia Red ﬁeld Michael Collins Ankur Parikh Chris Alberti Danielle Epstein Illia Polosukhin Jacob Devlin Kenton Lee et al. 2019. Natural questions a bench mark for question answering research. Transac tions of the Association of Computational Linguis tics TACL . Quoc Le and Tomas Mikolov. 2014. Distributed repre sentations of sentences and documents. In Interna tional Conference on Machine Learning ICML . Jinhyuk Lee Minjoon Seo Hannaneh Hajishirzi and Jaewoo Kang. 2020. Contextualized sparse repre sentations for real time open domain question an swering. In Association for Computational Linguis tics ACL . Kenton Lee Ming Wei Chang and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Association for Com putational Linguistics ACL . Kenton Lee Shimi Salant Tom Kwiatkowski Ankur Parikh Dipanjan Das and Jonathan Berant. 2017. Learning recurrent span representations for extrac tive question answering. In ICLR. Omer Levy Minjoon Seo Eunsol Choi and Luke Zettlemoyer. 2017. Zero shot relation extraction via reading comprehension. In Computational Natural Language Learning CoNLL . Patrick Lewis Ethan Perez Aleksandara Piktus Fabio Petroni Vladimir Karpukhin Naman Goyal Hein rich Küttler Mike Lewis Wen tau Yih Tim Rock täschel et al. 2020. Retrieval augmented generation for knowledge intensive nlp tasks. In Advances in Neural Information Processing Systems NeurIPS . Jimmy Lin Rodrigo Nogueira and Andrew Yates. 2020. Pretrained Transformers for text rank ing BERT and beyond. arXiv preprint arXiv 2010.06467. Lucian Vlad Lita Abe Ittycheriah Salim Roukos and Nanda Kambhatla. 2003. tRuEcasIng. In Associa tion for Computational Linguistics ACL . Tomas Mikolov Ilya Sutskever Kai Chen Greg S Cor rado and Jeff Dean. 2013. Distributed representa tions of words and phrases and their compositional ity. In Advances in Neural Information Processing Systems NIPS . Sewon Min Danqi Chen Hannaneh Hajishirzi and Luke Zettlemoyer. 2019. A discrete hard EM ap proach for weakly supervised question answering. In Empirical Methods in Natural Language Process ing EMNLP . Matthew E Peters Mark Neumann Mohit Iyyer Matt Gardner Christopher Clark Kenton Lee and Luke Zettlemoyer. 2018. Deep contextualized word repre sentations. In North American Chapter of the Asso ciation for Computational Linguistics NAACL . Fabio Petroni Aleksandra Piktus Angela Fan Patrick Lewis Majid Yazdani Nicola De Cao James Thorne Yacine Jernite Vassilis Plachouras Tim Rocktäschel et al. 2021. KILT a benchmark for knowledge intensive language tasks. In North Amer ican Chapter of the Association for Computational Linguistics NAACL . Colin Raffel Noam Shazeer Adam Roberts Katherine Lee Sharan Narang Michael Matena Yanqi Zhou Wei Li and Peter J Liu. 2020. Exploring the lim its of transfer learning with a uniﬁed text to text transformer. Journal of Machine Learning Research 21 140 . Pranav Rajpurkar Jian Zhang Konstantin Lopyrev and Percy Liang. 2016. SQuAD 100 000 questions for machine comprehension of text. In Empirical Meth ods in Natural Language Processing EMNLP . Adam Roberts Colin Raffel and Noam Shazeer. 2020. How much knowledge can you pack into the param eters of a language model In Empirical Methods in Natural Language Processing EMNLP . Minjoon Seo Tom Kwiatkowski Ankur Parikh Ali Farhadi and Hannaneh Hajishirzi. 2018. Phrase indexed question answering A new challenge for scalable document comprehension. In Empirical Methods in Natural Language Processing EMNLP .Minjoon Seo Jinhyuk Lee Tom Kwiatkowski Ankur P Parikh Ali Farhadi and Hannaneh Ha jishirzi. 2019. Real time open domain question an swering with dense sparse phrase index. In Associa tion for Computational Linguistics ACL . Wissam Siblini Mohamed Challal and Charlotte Pasqual. 2020. Delaying interaction layers in transformer based encoders for efﬁcient open domain question answering. arXiv preprint arXiv 2010.08422. Richard Socher Christopher D Manning and An drew Y Ng. 2010. Learning continuous phrase repre sentations and syntactic parsing with recursive neu ral networks. In Proceedings of the NIPS 2010 deep learning and unsupervised feature learning work shop. Ellen M Voorhees et al. 1999. The TREC 8 question answering track report. In Trec. Zhiguo Wang Patrick Ng Xiaofei Ma Ramesh Nallap ati and Bing Xiang. 2019. Multi passage BERT A globally normalized BERT model for open domain question answering. In Empirical Methods in Natu ral Language Processing EMNLP . Wei Yang Yuqing Xie Aileen Lin Xingyu Li Luchen Tan Kun Xiong Ming Li and Jimmy Lin. 2019. End to end open domain question answering with bertserini. In North American Chapter of the Asso ciation for Computational Linguistics NAACL . Wen tau Yih Kristina Toutanova John C Platt and Christopher Meek. 2011. Learning discriminative projections for text similarity measures. In Compu tational Natural Language Learning CoNLL .A Computational Cost We describe the resources and time spent dur ing inference Table 1 and A.1 and indexing Ta ble A.1 . With our limited GPU resources 24GB 4 it takes about 20 hours for indexing the entire phrase representations. We also largely reduced the storage from 1 547GB to 320GB by 1 removing sparse representations and 2 using our sharing and split strategy. See Appendix E for the details on the reduction of storage footprint and Appendix B for the speciﬁcation of our server for the benchmark. Indexing Resources Storage Time DPR 32GB GPU 8 76GB 17h DenSPI Sparc 24GB GPU 4 1 547GB 85h DensePhrases 24GB GPU 4 320GB 20h Inference RAM GPU Q sec GPU CPU DPR 86GB 17GB 0.9 0.04 DenSPI Sparc 27GB 2GB 2.1 1.7 DensePhrases 12GB 2GB 20.6 13.6 Table A.1 Complexity analysis of three open domain QA models during indexing and inference. For infer ence we also report the minimum requirement of RAM and GPU memory for running each model with GPU. For computing Q s for CPU we do not use GPUs but load all models on the RAM. B Server Speciﬁcations for Benchmark To compare the complexity of open domain QA models we install all models in Table 1 on the same server using their public open source code. Our server has the following speciﬁcations Hardware Intel Xeon CPU E5 2630 v4 @ 2.20GHz 128GB RAM 12GB GPU TITAN Xp 2 2TB 970 EVO Plus NVMe M.2 SSD 1 Table B.2 Server speciﬁcation for the benchmark For DPR due to its large memory consumption we use a similar server with a 24GB GPU TITAN RTX . For all models we use 1 000 randomly sam pled questions from the Natural Questions devel opment set for the speed benchmark and measure Q sec. We set the batch size to 64 for all models except BERTSerini ORQA and REALM which do not allow a batch size of more than 1 in their open source implementations. Q sec for DPR in cludes retrieving passages and running a reader Dataset Train Dev Test Natural Questions 79 168 8 757 3 610 WebQuestions 3 417 361 2 032 CuratedTrec 1 353 133 694 TriviaQA 78 785 8 837 11 313 SQuAD 78 713 8 886 10 570 T REx 2 284 168 5 000 5 000 Zero Shot RE 147 909 3 724 4 966 Table C.3 Statistics of ﬁve open domain QA datasets and two slot ﬁlling datasets. We follow the same splits in open domain QA for the two reading comprehension datasets SQuAD and Natural Questions . model and the batch size for the reader model is set to 8 to ﬁt in the 24GB GPU retriever batch size is still 64 . For other hyperparameters we use the default settings of each model. We also exclude the time and the number of questions in the ﬁrst ﬁve iterations for warming up each model. Note that despite our effort to match the environment of each model their latency can be affected by various dif ferent settings in their implementations such as the choice of library PyTorch vs. Tensorﬂow . C Data Statistics and Pre processing In Table C.3 we show the statistics of ﬁve open domain QA datasets and two slot ﬁlling datasets. Pre processed open domain QA datasets are pro vided by Chen et al. 2017 except Natural Ques tions and TriviaQA. We use a version of Natural Questions and TriviaQA provided by Min et al. 2019 Lee et al. 2019 which are pre processed for the open domain QA setting. Slot ﬁlling datasets are provided by Petroni et al. 2021 . We use two reading comprehension datasets SQuAD and Natural Questions for training our model on Eq. 9 . For SQuAD we use the original dataset provided by the authors Rajpurkar et al. 2016 . For Natural Questions Kwiatkowski et al. 2019 we use the pre processed version provided by Asai et al. 2020 .9 We use the short answer as a ground truth answer a and its long answer as a gold pas sage p. We also match the gold passages in Natural Questions to the paragraphs in Wikipedia whenever possible. Since we want to check the performance changes of our model with the growing number of tokens we follow the same split train dev test used in Natural Questions Open for the reading comprehension setting as well. During the valida 9https github.com AkariAsai learning_to_retrieve_reasoning_pathstion of our model and baseline models we exclude samples whose answers lie in a list or a table from a Wikipedia article. D Hyperparameters We use the Adam optimizer Kingma and Ba 2015 in all our experiments. For training our phrase and question encoders with Eq. 9 we use a learning rate of 3e 5 and the norm of the gradient is clipped at 1. We use a batch size of B 84 and train each model for 4 epochs for all datasets where the loss of pre batch negatives is applied in the last two epochs. We use SQuAD to train our QG model10 and use spaCy11 for extracting named entities in each training passage which are used to generate questions. The number of generated questions is 327 302 and 1 126 354 for SQuAD and Natural Questions respectively. The number of preceding batches C is set to 2. For the query side ﬁne tuning with Eq. 11 we use a learning rate of 3e 5 and the norm of the gra dient is clipped at 1. We use a batch size of 12 and train each model for 10 epochs for all datasets. The top k for the Eq. 11 is set to 100. While we use a single 24GB GPU TITAN RTX for train ing the phrase encoders with Eq. 9 query side ﬁne tuning is relatively cheap and uses a single 12GB GPU TITAN Xp . Using the development set we select the best performing model based on EM for each dataset which are then evaluated on each test set. Since SpanBERT only supports cased models we also truecase the questions Lita et al. 2003 that are originally provided in the lowercase Natural Questions and WebQuestions . E Reducing Storage Footprint As shown in Table 1 we have reduced the stor age footprint from 1 547GB Lee et al. 2020 to 320GB. We detail how we can reduce the storage footprint in addition to the several techniques intro duced by Seo et al. 2019 . First following Seo et al. 2019 we apply a linear transformation on the passage token repre sentations to obtain a set of ﬁlter logits which can be used to ﬁlter many token representations from W D . This ﬁlter layer is supervised by applying the binary cross entropy with the gold start end 10The quality of generated questions from a QG model trained on Natural Questions is worse due to the ambiguity of information seeking questions. 11https spacy.io positions trained together with Eq. 9 . We tune the threshold for the ﬁlter logits on the reading comprehension development set to the point where the performance does not drop signiﬁcantly while maximally ﬁltering tokens. In the full Wikipedia setting we ﬁlter about 75 of tokens and store 770M token representations. Second in our architecture we use a base model SpanBERT base for a smaller dimension of token representations d 768 and does not use any sparse representations including tf idf or contex tualized sparse representations Lee et al. 2020 . We also use the scalar quantization for storing float32 vectors as int4 during indexing. Lastly since the inference in Eq. 10 is purely based on MIPS we do not have to keep the original start and end vectors which takes about 500GB. However when we perform query side ﬁne tuning we need the original start and end vectors for re constructing them to compute Eq. 11 since the on disk version of MIPS index only returns the top k scores and their indices but not the vectors.