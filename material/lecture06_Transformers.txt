Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Self Attention and TransformersLecture Plan 2 1. From recurrence RNN to attention based NLP models 2. The Transformer model 3. Great results with Transformers Reminders Assignment 1 is due today Assignment 2 will be out todayAs of last lecture recurrent models for most NLP Circa 2016 the de facto strategy in NLP is to encode sentences with a bidirectional LSTM for example the source sentence in a translation Define your output parse sentence summary as a sequence and use an LSTM to generate it. Use attention to allow flexible access to memory 3Today Same goals different building blocks So far we learned about sequence to sequence problems and encoder decoder models. Today we re not trying to motivate entirely new ways of looking at problems like Machine Translation Instead we re trying to find the best building blocks to plug into our models and enable broad progress. 2014 2017ish Recurrence Lots of trial and error 2021 4Issues with recurrent models Linear interaction distance RNNs are unrolled left to right . This encodes linear locality a useful heuristic Nearby words often affect each other s meanings Problem RNNs take O sequence length steps for distant word pairs to interact. O sequence length tasty pizza The chef who was 5Issues with recurrent models Linear interaction distance O sequence length steps for distant word pairs to interact means Hard to learn long distance dependencies because gradient problems Linear order of words is baked in we already know linear order isn t the right way to think about sentences The chef who was Info of chef has gone through O sequence length many layers 6Issues with recurrent models Lack of parallelizability Forward and backward passes have O sequence length unparallelizable operations GPUs can perform a bunch of independent computations at once But future RNN hidden states can t be computed in full before past RNN hidden states have been computed Inhibits training on very large datasets h1 h2 0 1 n hT 1 7 2 2 3 Numbers indicate min of steps before a state can be computedIf not recurrence then what How about attention Attention treats each word s representation as a query to access and incorporate information from a set of values. We saw attention from the decoder to the encoder today we ll think about attention within a single sentence. Number of unparallelizable operations does not increase with sequence length. Maximum interaction distance O 1 since all words interact at every layer attention embedding h1 h2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 hT attention All words attend to all words in previous layer most arrows here are omitted 8Attention as a soft averaging lookup table We can think of attention as performing fuzzy lookup in a key value store. In a lookup table we have a table of keys that map to values. The query matches one of the keys returning its value. In attention the query matches all keys softly to a weight between 0 and 1. The keys values are multiplied by the weights and summed. 9Self Attention Hypothetical Example 10 I went to McMaster SEP 775 and learnedSelf Attention keys queries values from the same sequence Let 𝒘1 𝑛be a sequence of words in vocabulary 𝑉 like Zuko made his uncle tea. For each 𝒘𝑖 let 𝒙𝑖 𝐸𝒘𝒊 where 𝐸 ℝ𝑑 𝑉 is an embedding matrix. 1. Transform each word embedding with weight matrices Q K V each in ℝ𝑑 𝑑 2. Compute pairwise similarities between keys and queries normalize with softmax 3. Compute output for each word as weighted sum of values queries 𝒌𝑖 𝐾𝒙𝒊 keys 𝒒𝑖 𝑄𝒙𝒊 𝒗𝑖 𝑉𝒙𝒊 values 11Barriers Doesn t have an inherent notion of order Barriers and solutions for Self Attention as a building block Solutions 12Fixing the first self attention problem sequence order 13 Since self attention doesn t build in order information we need to encode the order of the sentence in our keys queries and values. Consider representing each sequence index as a vector 𝒑𝑖 ℝ𝑑 for 𝑖 1 2 𝑛 are position vectors Don t worry about what the 𝑝𝑖are made of yet Easy to incorporate this info into our self attention block just add the 𝒑𝑖to our inputs Recall that 𝒙𝑖is the embedding of the word at index 𝑖. The positioned embedding is In deep self attention networks we do this at the first layer You could concatenate them as well but people mostly just addPros Periodicity indicates that maybe absolute position isn t as important Maybe can extrapolate to longer sequences as periods restart Cons Not learnable also the extrapolation doesn t really work Sinusoidal position representations concatenate sinusoidal functions of varying periods sin 𝑖 100002 1 𝑑 cos 𝑖 100002 1 𝑑 𝒑𝑖 𝑑 Position representation vectors through sinusoids sin 𝑖 100002 2 𝑑 𝑑 cos 𝑖 100002 2 𝑑 Image https timodenk.com blog linear relationships in the transformers positional encoding Index in the sequence Dimension 14Learned absolute position representations Let all 𝑝𝑖be learnable parameters Learn a matrix 𝒑 ℝ𝑑 𝑛 and let each 𝒑𝑖be a column of that matrix Pros Flexibility each position gets to be learned to fit the data Cons Definitely can t extrapolate to indices outside 1 𝑛. Most systems use this Sometimes people try more flexible representations of position Relative linear position attention Shaw et al. 2018 Dependency syntax based position Wang et al. 2019 15 Position representation vectors learned from scratchBarriers Doesn t have an inherent notion of order No nonlinearities for deep learning It s all just weighted averages Barriers and solutions for Self Attention as a building block Solutions Add position representations to the inputs 16Adding nonlinearities in self attention Note that there are no elementwise nonlinearities in self attention stacking more self attention layers just re averages value vectors Why Easy fix add a feed forward network to post process each output vector. 𝑚𝑖 𝑀𝐿𝑃output𝑖 𝑊2 ReLU 𝑊1 output𝑖 𝑏1 𝑏2 𝑤1 The 𝑤2 chef 𝑤3 who 𝑤𝑛 food self attention Intuition the FF network processes the result of attention FF FF FF FF self attention FF FF FF FF 17Barriers Doesn t have an inherent notion of order No nonlinearities for deep learning magic It s all just weighted averages Need to ensure we don t look at the future when predicting a sequence Like in machine translation Or language modeling Barriers and solutions for Self Attention as a building block Solutions Add position representations to the inputs Easy fix apply the same feedforward network to each self attention output. 18Masking the future in self attention To use self attention in decoders we need to ensure we can t peek at the future. At every timestep we could change the set of keys and queries to include only past words. Inefficient To enable parallelization we mask out attention to future words by setting attention scores to . The chef who ST ART For encoding these words We can look at these not greyed out words 19Barriers Doesn t have an inherent notion of order No nonlinearities for deep learning magic It s all just weighted averages Need to ensure we don t look at the future when predicting a sequence Like in machine translation Or language modeling Barriers and solutions for Self Attention as a building block Solutions Add position representations to the inputs Easy fix apply the same feedforward network to each self attention output. Mask out the future by artificially setting attention weights to 0 20Self attention the basis of the method. Position representations Specify the sequence order since self attention is an unordered function of its inputs. Nonlinearities At the output of the self attention block Frequently implemented as a simple feed forward network. Masking In order to parallelize operations while not looking at the future. Keeps information about the future from leaking to the past. Necessities for a self attention building block 21Outline 22 1. From recurrence RNN to attention based NLP models 2. The Transformer model 3. Great results with TransformersThe Transformer Decoder A Transformer decoder is how we ll build systems like language models. It s a lot like our minimal self attention architecture but with a few more components. The embeddings and position embeddings are identical. We ll next replace our self attention with multi head self attention. Transformer Decoder 23Recall the Self Attention Hypothetical Example 24 I went to McMaster SEP 775 and learnedHypothetical Example of Multi Head Attention 25 I went to McMaster SEP 775 and learned I went to McMaster SEP 775 and learnedSequence Stacked form of Attention Let s look at how key query value attention is computed in matrices. Let 𝑋 𝑥1 𝑥𝑛 ℝ𝑛 𝑑be the concatenation of input vectors. First note that 𝑋𝐾 ℝ𝑛 𝑑 𝑋𝑄 ℝ𝑛 𝑑 𝑋𝑉 ℝ𝑛 𝑑. 𝖳 The output is defined as output softmax 𝑋𝑄𝑋𝐾 𝑋𝑉 ℝ𝑛 𝑑. 𝑋𝑄𝐾𝖳𝑋𝖳 ℝ𝑛 𝑛 All pairs of attention scores output ℝ𝑛 𝑑 𝐾𝖳𝑋𝖳 𝑋𝑄 First take the query key dot products in one matrix multiplication 𝑋𝑄𝑋𝐾𝖳 Next softmax and compute the weighted average with another matrix multiplication. 𝑋𝑄𝐾𝖳𝑋𝖳 softmax 𝑋𝑉 26Multi headed attention What if we want to look in multiple places in the sentence at once 𝑖 For word 𝑖 self attention looks where 𝑥𝖳𝑄𝖳𝐾𝑥𝑗is high but maybe we want to focus on different 𝑗for different reasons We ll define multiple attention heads through multiple Q K V matrices 𝑑 Let 𝑄𝑃 𝐾𝑃 𝑉 𝑃 ℝ ℎwhere ℎis the number of attention heads and ranges from 1 to ℎ. Each attention head performs attention independently Then the outputs of all the heads are combined output output1 outputℎ𝑌 where 𝑌 ℝ𝑑 𝑑 Each head gets to look at different things and construct value vectors differently. 27Multi head self attention is computationally efficient Even though we compute ℎmany attention heads it s not really more costly. We compute 𝑋𝑄 ℝ𝑛 𝑑 and then reshape to ℝ𝑛 ℎ 𝑑 ℎ. Likewise for 𝑋𝐾 𝑋𝑉. Then we transpose to ℝℎ 𝑛 𝑑 ℎ now the head axis is like a batch axis. Almost everything else is identical and the matrices are the same sizes. 28 𝑋𝑄 First take the query key dot products in one matrix multiplication 𝑋𝑄𝑋𝐾𝖳 𝐾𝖳𝑋𝖳 Next softmax and compute the weighted average with another matrix multiplication. softmax 𝑋𝑄𝐾𝖳𝑋𝖳 𝑋 𝑋 𝑉 𝑉 output ℝ𝑛 𝑑 𝑃 mix ℝ3 𝑛 𝑛 3 sets of all pairs of attention scores 𝑋𝑄𝐾𝖳𝑋𝖳The Transformer Decoder Now that we ve replaced self attention with multi head self attention we ll go through two optimization tricks that end up being Residual Connections Layer Normalization In most Transformer diagrams these are often written together as Add Norm Transformer Decoder 29The Transformer Encoder Residual connections He et al. 2016 Residual connections are a trick to help models train better. Instead of 𝑋 𝑖 Layer 𝑋𝑖 1 where 𝑖represents the layer We let 𝑋 𝑖 𝑋 𝑖 1 Layer 𝑋𝑖 1 so we only have to learn the residual from the previous layer Gradient is great through the residual connection it s 1 Bias towards the identity function Layer 𝑋 𝑖 1 𝑋 𝑖 Layer 𝑋 𝑖 1 𝑋 𝑖 no residuals residuals Loss landscape visualization Li et al. 2018 on a ResNet 31The Transformer Encoder Layer normalization Ba et al. 2016 𝑗 1 Layer normalization is a trick to help models train faster. Idea cut down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation within each layer. LayerNorm s success may be due to its normalizing gradients Xu et al. 2019 Let 𝑥 ℝ𝑑be an individual word vector in the model. Let 𝜇 σ𝑑 𝑥𝑗 this is the mean 𝜇 ℝ. 1 𝑑 𝑗 1 σ𝑑 𝑗 2 𝑥 𝜇 this is the standard deviation 𝜎 ℝ. Let 𝜎 Let 𝛾 ℝ𝑑and 𝛽 ℝ𝑑be learned gain and bias parameters. Can omit Then layer normalization computes 𝑥 𝜇 𝜎 𝜖 output 𝛾 𝛽 Normalize by scalar mean and variance Modulate by learned elementwise gain and bias 32The Transformer Decoder The Transformer Decoder is a stack of Transformer Decoder Blocks. Each Block consists of Self attention Add Norm Feed Forward Add Norm That s it We ve gone through the Transformer Decoder. Transformer Decoder 32The Transformer Encoder Transformer Decoder The Transformer Decoder constrains to unidirectional context as for language models. What if we want bidirectional context like in a bidirectional RNN This is the Transformer Encoder. The only difference is that we remove the masking in the self attention. No Masking 33The Transformer Encoder Decoder 35 Recall that in machine translation we processed the source sentence with a bidirectional model and generated the target with a unidirectional model. For this kind of seq2seq format we often use a Transformer Encoder Decoder. We use a normal Transformer Encoder. Our Transformer Decoder is modified to perform cross attention to the output of the Encoder.Outline 38 1. From recurrence RNN to attention based NLP models 2. Introducing the Transformer model 3. Great results with TransformersGreat Results with Transformers Not just better Machine Also more efficient to Translation BLEU scores train First Machine Translation from the original Transformers paper Vaswani et al. 2017 Test sets WMT 2014 English German and English French 39Great Results with Transformers Transformers all the way down. Next document generation The old standard Liu et al. 2018 WikiSum dataset 40Great Results with Transformers Liu et al. 2018 Before too long most Transformers results also included pretraining. Transformers parallelizability allows for efficient pretraining and have made them the de facto standard. On this popular aggregate benchmark for example All top models are Transformer and pretraining based. More results Thursday when we discuss pretraining. 41Questions