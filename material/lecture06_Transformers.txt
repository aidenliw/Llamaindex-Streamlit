Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Self Attention and TransformersLecture Plan 2 1. From recurrence RNN to attention based NLP models 2. The Transformer model 3. Great results with Transformers Reminders Assignment 1 is due today Assignment 2 will be out todayAs of last lecture recurrent models for most NLP Circa 2016 the de facto strategy in NLP is to encode sentences with a bidirectional LSTM for example the source sentence in a translation Define your output parse sentence summary as a sequence and use an LSTM to generate it. Use attention to allow flexible access to memory 3Today Same goals different building blocks So far we learned about sequence to sequence problems and encoder decoder models. Today we re not trying to motivate entirely new ways of looking at problems like Machine Translation Instead we re trying to find the best building blocks to plug into our models and enable broad progress. 2014 2017ish Recurrence Lots of trial and error 2021 4Issues with recurrent models Linear interaction distance RNNs are unrolled left to right . This encodes linear locality a useful heuristic Nearby words often affect each other s meanings Problem RNNs take O sequence length steps for distant word pairs to interact. O sequence length tasty pizza The chef who was 5Issues with recurrent models Linear interaction distance O sequence length steps for distant word pairs to interact means Hard to learn long distance dependencies because gradient problems Linear order of words is baked in we already know linear order isn t the right way to think about sentences The chef who was Info of chef has gone through O sequence length many layers 6Issues with recurrent models Lack of parallelizability Forward and backward passes have O sequence length unparallelizable operations GPUs can perform a bunch of independent computations at once But future RNN hidden states can t be computed in full before past RNN hidden states have been computed Inhibits training on very large datasets h1 h2 0 1 n hT 1 7 2 2 3 Numbers indicate min of steps before a state can be computedIf not recurrence then what How about attention Attention treats each word s representation as a query to access and incorporate information from a set of values. We saw attention from the decoder to the encoder today we ll think about attention within a single sentence. Number of unparallelizable operations does not increase with sequence length. Maximum interaction distance O 1 since all words interact at every layer attention embedding h1 h2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 hT attention All words attend to all words in previous layer most arrows here are omitted 8Attention as a soft averaging lookup table We can think of attention as performing fuzzy lookup in a key value store. In a lookup table we have a table of keys that map to values. The query matches one of the keys returning its value. In attention the query matches all keys softly to a weight between 0 and 1. The keys values are multiplied by the weights and summed. 9Self Attention Hypothetical Example 10 I went to McMaster SEP 775 and learnedSelf Attention keys queries values from the same sequence Let ğ’˜1 ğ‘›be a sequence of words in vocabulary ğ‘‰ like Zuko made his uncle tea. For each ğ’˜ğ‘– let ğ’™ğ‘– ğ¸ğ’˜ğ’Š where ğ¸ â„ğ‘‘ ğ‘‰ is an embedding matrix. 1. Transform each word embedding with weight matrices Q K V each in â„ğ‘‘ ğ‘‘ 2. Compute pairwise similarities between keys and queries normalize with softmax 3. Compute output for each word as weighted sum of values queries ğ’Œğ‘– ğ¾ğ’™ğ’Š keys ğ’’ğ‘– ğ‘„ğ’™ğ’Š ğ’—ğ‘– ğ‘‰ğ’™ğ’Š values 11Barriers Doesn t have an inherent notion of order Barriers and solutions for Self Attention as a building block Solutions 12Fixing the first self attention problem sequence order 13 Since self attention doesn t build in order information we need to encode the order of the sentence in our keys queries and values. Consider representing each sequence index as a vector ğ’‘ğ‘– â„ğ‘‘ for ğ‘– 1 2 ğ‘› are position vectors Don t worry about what the ğ‘ğ‘–are made of yet Easy to incorporate this info into our self attention block just add the ğ’‘ğ‘–to our inputs Recall that ğ’™ğ‘–is the embedding of the word at index ğ‘–. The positioned embedding is In deep self attention networks we do this at the first layer You could concatenate them as well but people mostly just addPros Periodicity indicates that maybe absolute position isn t as important Maybe can extrapolate to longer sequences as periods restart Cons Not learnable also the extrapolation doesn t really work Sinusoidal position representations concatenate sinusoidal functions of varying periods sin ğ‘– 100002 1 ğ‘‘ cos ğ‘– 100002 1 ğ‘‘ ğ’‘ğ‘– ğ‘‘ Position representation vectors through sinusoids sin ğ‘– 100002 2 ğ‘‘ ğ‘‘ cos ğ‘– 100002 2 ğ‘‘ Image https timodenk.com blog linear relationships in the transformers positional encoding Index in the sequence Dimension 14Learned absolute position representations Let all ğ‘ğ‘–be learnable parameters Learn a matrix ğ’‘ â„ğ‘‘ ğ‘› and let each ğ’‘ğ‘–be a column of that matrix Pros Flexibility each position gets to be learned to fit the data Cons Definitely can t extrapolate to indices outside 1 ğ‘›. Most systems use this Sometimes people try more flexible representations of position Relative linear position attention Shaw et al. 2018 Dependency syntax based position Wang et al. 2019 15 Position representation vectors learned from scratchBarriers Doesn t have an inherent notion of order No nonlinearities for deep learning It s all just weighted averages Barriers and solutions for Self Attention as a building block Solutions Add position representations to the inputs 16Adding nonlinearities in self attention Note that there are no elementwise nonlinearities in self attention stacking more self attention layers just re averages value vectors Why Easy fix add a feed forward network to post process each output vector. ğ‘šğ‘– ğ‘€ğ¿ğ‘ƒoutputğ‘– ğ‘Š2 ReLU ğ‘Š1 outputğ‘– ğ‘1 ğ‘2 ğ‘¤1 The ğ‘¤2 chef ğ‘¤3 who ğ‘¤ğ‘› food self attention Intuition the FF network processes the result of attention FF FF FF FF self attention FF FF FF FF 17Barriers Doesn t have an inherent notion of order No nonlinearities for deep learning magic It s all just weighted averages Need to ensure we don t look at the future when predicting a sequence Like in machine translation Or language modeling Barriers and solutions for Self Attention as a building block Solutions Add position representations to the inputs Easy fix apply the same feedforward network to each self attention output. 18Masking the future in self attention To use self attention in decoders we need to ensure we can t peek at the future. At every timestep we could change the set of keys and queries to include only past words. Inefficient To enable parallelization we mask out attention to future words by setting attention scores to . The chef who ST ART For encoding these words We can look at these not greyed out words 19Barriers Doesn t have an inherent notion of order No nonlinearities for deep learning magic It s all just weighted averages Need to ensure we don t look at the future when predicting a sequence Like in machine translation Or language modeling Barriers and solutions for Self Attention as a building block Solutions Add position representations to the inputs Easy fix apply the same feedforward network to each self attention output. Mask out the future by artificially setting attention weights to 0 20Self attention the basis of the method. Position representations Specify the sequence order since self attention is an unordered function of its inputs. Nonlinearities At the output of the self attention block Frequently implemented as a simple feed forward network. Masking In order to parallelize operations while not looking at the future. Keeps information about the future from leaking to the past. Necessities for a self attention building block 21Outline 22 1. From recurrence RNN to attention based NLP models 2. The Transformer model 3. Great results with TransformersThe Transformer Decoder A Transformer decoder is how we ll build systems like language models. It s a lot like our minimal self attention architecture but with a few more components. The embeddings and position embeddings are identical. We ll next replace our self attention with multi head self attention. Transformer Decoder 23Recall the Self Attention Hypothetical Example 24 I went to McMaster SEP 775 and learnedHypothetical Example of Multi Head Attention 25 I went to McMaster SEP 775 and learned I went to McMaster SEP 775 and learnedSequence Stacked form of Attention Let s look at how key query value attention is computed in matrices. Let ğ‘‹ ğ‘¥1 ğ‘¥ğ‘› â„ğ‘› ğ‘‘be the concatenation of input vectors. First note that ğ‘‹ğ¾ â„ğ‘› ğ‘‘ ğ‘‹ğ‘„ â„ğ‘› ğ‘‘ ğ‘‹ğ‘‰ â„ğ‘› ğ‘‘. ğ–³ The output is defined as output softmax ğ‘‹ğ‘„ğ‘‹ğ¾ ğ‘‹ğ‘‰ â„ğ‘› ğ‘‘. ğ‘‹ğ‘„ğ¾ğ–³ğ‘‹ğ–³ â„ğ‘› ğ‘› All pairs of attention scores output â„ğ‘› ğ‘‘ ğ¾ğ–³ğ‘‹ğ–³ ğ‘‹ğ‘„ First take the query key dot products in one matrix multiplication ğ‘‹ğ‘„ğ‘‹ğ¾ğ–³ Next softmax and compute the weighted average with another matrix multiplication. ğ‘‹ğ‘„ğ¾ğ–³ğ‘‹ğ–³ softmax ğ‘‹ğ‘‰ 26Multi headed attention What if we want to look in multiple places in the sentence at once ğ‘– For word ğ‘– self attention looks where ğ‘¥ğ–³ğ‘„ğ–³ğ¾ğ‘¥ğ‘—is high but maybe we want to focus on different ğ‘—for different reasons We ll define multiple attention heads through multiple Q K V matrices ğ‘‘ Let ğ‘„ğ‘ƒ ğ¾ğ‘ƒ ğ‘‰ ğ‘ƒ â„ â„where â„is the number of attention heads and ranges from 1 to â„. Each attention head performs attention independently Then the outputs of all the heads are combined output output1 outputâ„ğ‘Œ where ğ‘Œ â„ğ‘‘ ğ‘‘ Each head gets to look at different things and construct value vectors differently. 27Multi head self attention is computationally efficient Even though we compute â„many attention heads it s not really more costly. We compute ğ‘‹ğ‘„ â„ğ‘› ğ‘‘ and then reshape to â„ğ‘› â„ ğ‘‘ â„. Likewise for ğ‘‹ğ¾ ğ‘‹ğ‘‰. Then we transpose to â„â„ ğ‘› ğ‘‘ â„ now the head axis is like a batch axis. Almost everything else is identical and the matrices are the same sizes. 28 ğ‘‹ğ‘„ First take the query key dot products in one matrix multiplication ğ‘‹ğ‘„ğ‘‹ğ¾ğ–³ ğ¾ğ–³ğ‘‹ğ–³ Next softmax and compute the weighted average with another matrix multiplication. softmax ğ‘‹ğ‘„ğ¾ğ–³ğ‘‹ğ–³ ğ‘‹ ğ‘‹ ğ‘‰ ğ‘‰ output â„ğ‘› ğ‘‘ ğ‘ƒ mix â„3 ğ‘› ğ‘› 3 sets of all pairs of attention scores ğ‘‹ğ‘„ğ¾ğ–³ğ‘‹ğ–³The Transformer Decoder Now that we ve replaced self attention with multi head self attention we ll go through two optimization tricks that end up being Residual Connections Layer Normalization In most Transformer diagrams these are often written together as Add Norm Transformer Decoder 29The Transformer Encoder Residual connections He et al. 2016 Residual connections are a trick to help models train better. Instead of ğ‘‹ ğ‘– Layer ğ‘‹ğ‘– 1 where ğ‘–represents the layer We let ğ‘‹ ğ‘– ğ‘‹ ğ‘– 1 Layer ğ‘‹ğ‘– 1 so we only have to learn the residual from the previous layer Gradient is great through the residual connection it s 1 Bias towards the identity function Layer ğ‘‹ ğ‘– 1 ğ‘‹ ğ‘– Layer ğ‘‹ ğ‘– 1 ğ‘‹ ğ‘– no residuals residuals Loss landscape visualization Li et al. 2018 on a ResNet 31The Transformer Encoder Layer normalization Ba et al. 2016 ğ‘— 1 Layer normalization is a trick to help models train faster. Idea cut down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation within each layer. LayerNorm s success may be due to its normalizing gradients Xu et al. 2019 Let ğ‘¥ â„ğ‘‘be an individual word vector in the model. Let ğœ‡ Ïƒğ‘‘ ğ‘¥ğ‘— this is the mean ğœ‡ â„. 1 ğ‘‘ ğ‘— 1 Ïƒğ‘‘ ğ‘— 2 ğ‘¥ ğœ‡ this is the standard deviation ğœ â„. Let ğœ Let ğ›¾ â„ğ‘‘and ğ›½ â„ğ‘‘be learned gain and bias parameters. Can omit Then layer normalization computes ğ‘¥ ğœ‡ ğœ ğœ– output ğ›¾ ğ›½ Normalize by scalar mean and variance Modulate by learned elementwise gain and bias 32The Transformer Decoder The Transformer Decoder is a stack of Transformer Decoder Blocks. Each Block consists of Self attention Add Norm Feed Forward Add Norm That s it We ve gone through the Transformer Decoder. Transformer Decoder 32The Transformer Encoder Transformer Decoder The Transformer Decoder constrains to unidirectional context as for language models. What if we want bidirectional context like in a bidirectional RNN This is the Transformer Encoder. The only difference is that we remove the masking in the self attention. No Masking 33The Transformer Encoder Decoder 35 Recall that in machine translation we processed the source sentence with a bidirectional model and generated the target with a unidirectional model. For this kind of seq2seq format we often use a Transformer Encoder Decoder. We use a normal Transformer Encoder. Our Transformer Decoder is modified to perform cross attention to the output of the Encoder.Outline 38 1. From recurrence RNN to attention based NLP models 2. Introducing the Transformer model 3. Great results with TransformersGreat Results with Transformers Not just better Machine Also more efficient to Translation BLEU scores train First Machine Translation from the original Transformers paper Vaswani et al. 2017 Test sets WMT 2014 English German and English French 39Great Results with Transformers Transformers all the way down. Next document generation The old standard Liu et al. 2018 WikiSum dataset 40Great Results with Transformers Liu et al. 2018 Before too long most Transformers results also included pretraining. Transformers parallelizability allows for efficient pretraining and have made them the de facto standard. On this popular aggregate benchmark for example All top models are Transformer and pretraining based. More results Thursday when we discuss pretraining. 41Questions