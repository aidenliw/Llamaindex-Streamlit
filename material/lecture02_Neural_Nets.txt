Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Neural Networks Gradients and BackpropagationBinary classification for center word being location We do supervised training and want high score if it s a location 洧냫洧노洧랚 洧랥洧 1 1 洧 洧 x xmuseums xin xParis xare xamazing predicted model probability of class f Some element wise non linear function e.g. logistic tanh ReLU 2 R5d Embedding of 1 hot wordsNeural computation Original McCulloch Pitts 1943 threshold unit 洧릞 洧녥洧논 洧랚 洧릞 洧녥洧논 洧랚 0 This function has no slope so no gradient based learning 3tanh is just a rescaled and shifted sigmoid 2 as steep 1 1 tanh z 2logistic 2z 1 Logistic and tanh are still used e.g. logistic to get a probability However now for deep networks the first thing to try is ReLU it trains quickly and performs well due to good gradient backflow. ReLU has a negative dead zone that recent proposals mitigate GELU is frequently used with Transformers BERT RoBERTa etc. Non linearities old and new logistic sigmoid tanh hard tanh 1 0 1 1 Swish arXiv 1710.05941 swish 洧논 洧논. logistic 洧논 Rectified Linear Unit ReLU ReLU 洧녾 max 洧녾 0 Leaky ReLU Parametric ReLU 0 0 0 GELU arXiv 1606.08415 GELU 洧논 洧논. 洧녞洧녦 洧논 洧녦 洧녜 0 1 洧논. logistic 1.702洧논Non linearities i.e. f on previous slide Why they re needed Neural networks do function approximation e.g. regression or classification Without non linearities deep neural networks can t do anything more than a linear transform Extra layers could just be compiled down into a single linear transform W1 W2 x Wx But with more layers that include non linearities they can approximate any complex function 5Training with cross entropy loss you use this in PyTorch Until now our objective was stated as to maximize the probability of the correct class y or equivalently we can minimize the negative log probability of that class Now restated in terms of cross entropy a concept from information theory Let the true probability distribution be p let our computed model probability be q The cross entropy is Assuming a ground truth or true or gold or target probability distribution that is 1 at the right class and 0 everywhere else p 0 0 1 0 0 then Because of one hot p the only term left is the negative log probability of the true class yi log 洧녷 洧녽i 洧논i Cross entropy can be used in other ways with a more interesting p but for now just know that you ll want to use it as the loss in PyTorch 6Remember Stochastic Gradient Descent Update equation i.e. for each parameter In deep learning 洧랚includes the data representation e.g. word vectors too How can we compute 洧랚洧냫 洧랚 1. By hand 2. Algorithmically the backpropagation algorithm 洧띺 step size or learning rate 7Lecture Plan 8 Gradients 1. Introduction 2. Matrix calculus 3. BackpropagationComputing Gradients by Hand 9 Matrix calculus Fully vectorized gradients Multivariable calculus is just like single variable calculus if you use matrices Much faster and more useful than non vectorized gradients But doing a non vectorized gradient can be good for intuition recall the first lecture for an exampleGradients Given a function with 1 output and 1 input 洧녭洧논 洧논3 It s gradient slope is its derivative 洧녬f 3洧논2 10 洧녬洧논 How much will the output change if we change the input a bit At x 1 it changes about 3 times as much 1.013 1.03 At x 4 it changes about 48 times as much 4.013 64.48Gradients Given a function with 1 output and n inputs Its gradient is a vector of partial derivatives with respect to each input 11Jacobian Matrix Generalization of the Gradient Given a function with m outputs and n inputs It s Jacobian is an m x n matrix of partial derivatives 12Chain Rule For composition of one variable functions multiply derivatives For multiple variables functions multiply Jacobians 13Example Jacobian Elementwise activation Function 14Example Jacobian Elementwise activation Function Function has n outputs and n inputs n by n Jacobian 15Example Jacobian Elementwise activation Function 16Example Jacobian Elementwise activation Function 17Example Jacobian Elementwise activation Function 18Other Jacobians Compute these at home for practice Check your answers with the lecture notes 19Other Jacobians Compute these at home for practice Check your answers with the lecture notes 20Other Jacobians Compute these at home for practice Check your answers with the lecture notes Fine print This is the correct Jacobian. Later we discuss the shape convention using it the answer would be h. 21Other Jacobians Compute these at home for practice 22Back to our Neural Net x xmuseums xin xParis xare xamazing 23Back to our Neural Net Let s find Really we care about the gradient of the loss Jt but we will compute the gradient of the score for simplicity x xmuseums xin xParis xare xamazing 241. Break up equations into simple pieces Carefully define your variables and keep track of their dimensionality 252. Apply the chain rule 262. Apply the chain rule 272. Apply the chain rule 282. Apply the chain rule 293. Write out the Jacobians Useful Jacobians from previous slide 303. Write out the Jacobians 洧눘洧녢 Useful Jacobians from previous slide 313. Write out the Jacobians 洧눘洧녢 Useful Jacobians from previous slide 323. Write out the Jacobians 洧눘洧녢 Useful Jacobians from previous slide 333. Write out the Jacobians 洧눘洧녢 洧눘洧녢 Useful Jacobians from previous slide . 34 Hadamard product element wise multiplication of 2 vectors to give vectorRe using Computation Suppose we now want to compute Using the chain rule again 35Re using Computation Suppose we now want to compute Using the chain rule again The same Let s avoid duplicated computation 36Re using Computation Suppose we now want to compute Using the chain rule again 洧쯜s the upstream gradient error signal 37 洧눘洧녢Derivative with respect to Matrix Output shape What does look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do 38Derivative with respect to Matrix Output shape What does look like 1 output nm inputs 1 by nm Jacobian Inconvenient to then do Instead we leave pure math and use the shape convention the shape of the gradient is the shape of the parameters So is n by m 39Derivative with respect to Matrix What is is going to be in our answer The other term should be because Answer is 洧쯜s upstream gradient error signal at 洧녾 洧논is local input signal 40Why the Transposes Hacky answer this makes the dimensions work out Useful trick for checking your work Full explanation in the Goodflow book Each input goes to each output you want to get outer product 41Deriving local input gradient in backprop For in our equation x1 x2 f z1 h1 s u2 h2 f z2 W23 b2 x3 1 洧 洧럋 洧눝 洧럋 洧쮫눛 洧눆 洧 洧 洧 Let s consider the derivative of a single weight Wij Wij only contributes to zi For example W23is only used to compute z2 not z1 423. Backpropagation 43 We ve almost shown you backpropagation It s taking derivatives and using the generalized multivariate or matrix chain rule Other trick We re use derivatives computed for higher layers in computing derivatives for lower layers to minimize computationComputation Graphs and Backpropagation Software represents our neural net equations as a graph Source nodes inputs Interior nodes operations 44Computation Graphs and Backpropagation Software represents our neural net equations as a graph Source nodes inputs Interior nodes operations Edges pass along result of the operation 45Computation Graphs and Backpropagation Software represents our neural net equations as a graph Source nodes inputs Interior nodes operations Edges pass along result of the operation Forward Propagation 46Backpropagation Then go backwards along edges Pass along gradients 47Backpropagation Single Node Node receives an upstream gradient Goal is to pass on the correct downstream gradient Upstream gradient 51 Downstream gradientBackpropagation Single Node Each node has a local gradient The gradient of its output with respect to its input Upstream gradient Downstream gradient Local gradient 49Backpropagation Single Node Each node has a local gradient The gradient of its output with respect to its input Upstream gradient Downstream gradient Local gradient 50 Chain ruleBackpropagation Single Node Each node has a local gradient The gradient of its output with respect to its input downstream gradient upstream gradient x local gradient Upstream gradient Downstream gradient Local gradient 51Backpropagation Single Node 52 What about nodes with multiple inputsBackpropagation Single Node Downstream gradients 53 Upstream gradient Local gradients Multiple inputs multiple local gradientsAn Example 54An Example max Forward prop steps 55An Example max Forward prop steps 56 6 3 2 1 2 2 0An Example max Forward prop steps 6 3 2 1 2 2 0 Local gradients 57An Example max Forward prop steps 6 3 2 1 2 2 0 Local gradients 58An Example max Forward prop steps 6 3 2 1 2 2 0 Local gradients 59An Example max Forward prop steps 6 3 2 1 2 2 0 Local gradients 60An Example max Forward prop steps 1 2 2 0 Local gradients upstream local downstream 61 6 1 2 1 3 3 3 1 2 2An Example max Forward prop steps 1 2 6 1 2 3 3 2 2 3 1 3 Local gradients 0 3 0 0 upstream local downstream 62An Example max Forward prop steps Local gradients upstream local downstream 66 6 1 2 3 3 2 1 2 1 2 2 2 1 2 2 3 0 0An Example max Forward prop steps Local gradients 6 1 2 3 3 2 1 2 2 2 2 3 0 0 64Gradients sum at outward branches 65Gradients sum at outward branches 66Node Intuitions max 2 67 0 6 1 3 2 1 2 2 2 2 distributes the upstream gradient to each summandNode Intuitions max 3 68 1 2 6 1 2 3 2 3 0 0 distributes the upstream gradient to each summand max routes the upstream gradientNode Intuitions max 2 69 2 0 6 1 2 3 3 2 distributes the upstream gradient max routes the upstream gradient switches the upstream gradient 1Efficiency compute all gradients at once Incorrect way of doing backprop First compute 70Efficiency compute all gradients at once Incorrect way of doing backprop First compute Then independently compute Duplicated computation 71Efficiency compute all gradients at once Correct way Compute all the gradients at once Analogous to using 洧럋when we computed gradients by hand 721. Fprop visit nodes in topological sort order Compute value of node given predecessors 2. Bprop initialize output gradient 1 visit nodes in reverse order Compute gradient wrt each node using gradient wrt successors successors of Done correctly big O complexity of fprop and bprop is the same In general our nets have regular layer structure and so we can use matrices and Jacobians Back Prop in General Computation Graph Inputs Single scalar output 73Automatic Differentiation The gradient computation can be automatically inferred from the symbolic expression of the fprop Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output Modern DL frameworks Tensorflow PyTorch etc. do backpropagation for you but mainly leave layer node writer to hand calculate the local derivative 74Backprop Implementations 75Implementation forward backward API 76Implementation forward backward API 77Manual Gradient checking Numeric Gradient For small h 1e 4 Easy to implement correctly But approximate and very slow You have to recompute f for every parameter of our model Useful for checking your implementation In the old days we hand wrote everything doing this everywhere was the key test Now much less needed you can use it to check layers are correctly implemented 78Summary We ve mastered the core technology of neural nets Backpropagation recursively and hence efficiently apply the chain rule along computation graph downstream gradient upstream gradient x local gradient Forward pass compute results of operations and save intermediate values Backward pass apply chain rule to compute gradients 79Why learn all these details about gradients 80 Modern deep learning frameworks compute gradients for you But why take a class on compilers or systems when they are implemented for you Understanding what is going on under the hood is useful Backpropagation doesn t always work perfectly out of the box Understanding why is crucial for debugging and improving models See Karpathy article https medium.com @karpathy yes you should understand backprop e2f06eab496b Example in future lecture exploding and vanishing gradients