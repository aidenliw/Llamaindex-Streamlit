Hamidreza Mahyar mahyarh@mcmaster.ca Computational Natural Language Processing Language Models and Recurrent Neural NetsLecture Plan 1. A bit more about neural networks Language modeling RNNs 2. A new NLP task Language Modeling 4. Problems with RNNs 5. Recap on RNNs LMs Reminders You should have started working on Assignment 1 2 motivates 3. A new family of neural networks Recurrent Neural Networks RNNs This is the most important concept in the class It leads to GPT 3 and ChatGPT Important and used in Ass1 but not the only way to build LMs0 model power Training error Test error Classic view Regularization works to prevent overfitting when we have a lot of features or later a very powerful deep model etc. Now Regularization produces models that generalize well when we have a big model We do not care that our models overfit on the training data even though they are hugely overfit error overfitting We have models with many parameters Regularization 3 A full loss function includes regularization over all parameters ùúÉ e.g. L2 regularizationDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 4 Preventing Feature Co adaptation Good Regularization Method Use it everywhere Training time at each instance of evaluation in online SGD training randomly set 50 p of the inputs to each neuron to 0 Test time halve the model weights now twice as many Except usually only drop first layer inputs a little 15 or not at all This prevents feature co adaptation A feature cannot only be useful in the presence of particular other features In a single layer A kind of middle ground between Na√Øve Bayes where all feature weights are set independently and logistic regression models where weights are set in the context of all others Can be thought of as a form of model bagging i.e. like an ensemble model Nowadays usually thought of as strong feature dependent regularizer Wager Wang Liang 2013DropoutDropout Srivastava Hinton Krizhevsky Sutskever Salakhutdinov 2012 JMLR 2014 During training For each data point each time Randomly set input to 0 with probability ùëù dropout ratio often p 0.5 except p 0.15 for input layer via dropout mask During testing Multiply all weights by 1 ùëù No other dropout ùë•2 ùë•3 0 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë• ùë• 1 1 Train 1 Train 2 Test ùë•3 0 1 ùë¶ ùë§1ùë•1 ùë§3ùë•3 ùëè ùë§1 ùë§2 ùë§3 ùë§4 ùëè ùë•1 0 ùë•2 ùë•3 ùë•4 ùë§1 ùë§2 ùë§3 ùë§4 ùëè 1 ùë¶ 1 ùëù ùë§1ùë•1 ùë§2ùë•2 ùë§3ùë•3 ùë§4ùë•4 6Vectorization E.g. looping over word vectors versus concatenating them all into one large matrix and then multiplying the softmax weights with that matrix for loop 1000 loops best of 3 639 ¬µs per loop Using single a C x N matrix 10000 loops best of 3 53.8 ¬µs per loop Matrices are awesome Always try to use vectors and matrices rather than for loops The speed gain goes from 1 to 2 orders of magnitude with GPUs 7Parameter Initialization You normally must initialize weights to small random values i.e. not zero matrices To avoid symmetries that prevent learning specialization Initialize hidden layer biases to 0 and output or reconstruction biases to optimal value if weights were 0 e.g. mean target or inverse sigmoid of mean target Initialize all other weights Uniform r r with r chosen so numbers get neither too big or too small later the need for this is removed with use of layer normalization Xavier initialization has variance inversely proportional to fan in nin previous layer size and fan out nout next layer sizeOptimizers Usually plain SGD will work just fine However getting good results will often require hand tuning the learning rate E.g. start it higher and halve it every k epochs passes through full data shuffled or sampled For more complex nets or to avoid worry try more sophisticated adaptive optimizers that scale the adjustment to individual parameters by an accumulated gradient These models give differential per parameter learning rates Adagrad √ü Simplest member of family but tends to stall early RMSprop Adam √ü A fairly good safe place to begin in many cases AdamW NAdamW √ü Can be better with word vectors W and for speed Nesterov acceleration Start them with an initial learning rate around 0.001 √ü Many have other hyperparametersMore formally given a sequence of words compute the probability distribution of the next word where can be any word in the vocabulary A system that does this is called a Language Model 2. Language Modeling exams minds laptops Language Modeling is the task of predicting what word comes next books the students opened their 10Language Modeling You can also think of a Language Model as a system that assigns a probability to a piece of text For example if we have some text then the probability of this text according to the Language Model is This is what our LM provides 11You use Language Models every day 12You use Language Models every day 13n gram Language Models 14 the students opened their Question How to learn a Language Model Answer pre Deep Learning learn an n gram Language Model Definition An n gram is a chunk of n consecutive words. unigrams the students opened their bigrams the students students opened opened their trigrams the students opened students opened their four grams the students opened their Idea Collect statistics about how frequent different n grams are and use these to predict next word.n gram Language Models Question How do we get these n gram and n 1 gram probabilities Answer By counting them in some large corpus of text statistical approximation definition of conditional prob First we make a Markov assumption ùë• ùë° 1 depends only on the preceding n 1 words n 1 words assumption 15 prob of a n gram prob of a n 1 gramn gram Language Models Example Suppose we are learning a 4 gram Language Model. as the proctor started the clock the students opened their discard condition on this For example suppose that in the corpus students opened their occurred 1000 times students opened their books occurred 400 times √† P books students opened their 0.4 students opened their exams occurred 100 times √† P exams students opened their 0.1 Should we have discarded the proctor context 16Sparsity Problems with n gram Language Models Note Increasing n makes sparsity problems worse. Typically we can t have n bigger than 5. Problem What if students opened their never occurred in data Then we can t calculate probability for any ùë§ Sparsity Problem 2 Problem What if students opened their ùë§ never occurred in data Then ùë§has probability 0 Sparsity Problem 1 Partial Solution Add small ùõø to the count for every ùë§ ùëâ. This is called smoothing. Partial Solution Just condition on opened their instead. This is called backoff. 17Storage Problems with n gram Language Models 18 Storage Need to store count for all n grams you saw in the corpus. Increasing n or increasing corpus increases model sizen gram Language Models in practice You can build a simple trigram Language Model over a 1.7 million word corpus Reuters in a few seconds on your laptop today the Otherwise seems reasonable 0.153 0.153 company bank price italian emirate 0.077 0.039 0.039 get probability distribution Sparsity problem not much granularity in the probability distribution Business and financial news 19Generating text with a n gram Language Model You can also use a Language Model to generate text today the condition on this company 0.153 bank 0.153 price 0.077 italian 0.039 emirate 0.039 get probability distribution sample 20Generating text with a n gram Language Model You can also use a Language Model to generate text today the price condition on this of 0.308 for it to is 0.050 0.046 0.046 0.031 get probability distribution sample 21Generating text with a n gram Language Model You can also use a Language Model to generate text today the price of condition on this the 18 oil its gold 0.072 0.043 0.043 0.036 0.018 get probability distribution sample 22Generating text with a n gram Language Model 23 You can also use a Language Model to generate text today the price of gold per ton while production of shoe lasts and shoe industry the bank intervened just after it considered and rejected an imf demand to rebuild depleted european stocks sept 30 end primary 76 cts a share . Surprisingly grammatical but incoherent. We need to consider more than three words at a time if we want to model language well. But increasing n worsens sparsity problem and increases model sizeHow to build a neural language model Recall the Language Modeling task Input sequence of words Output prob. dist. of the next word How about a window based neural model We saw this applied to Named Entity Recognition in Lecture 2 LOCATION in Paris are amazing museums 24A fixed window neural Language Model their as the proctor started the clock discard the students opened fixed window 25A fixed window neural Language Model the students opened their books laptops concatenated word embeddings words one hot vectors hidden layer a zoo output distribution 26A fixed window neural Language Model the students opened their books laptops a zoo Improvements over n gram LM No sparsity problem Don t need to store all observed n grams 27 Remaining problems Fixed window is too small Enlarging window enlarges ùëä Window can never be large enough ùë• 1 and ùë• 2 are multiplied by completely different weights in ùëä. No symmetry in how the inputs are processed. We need a neural architecture that can process any length input Approximately Y. Bengio et al. 2000 2003 A Neural Probabilistic Language Model3. Recurrent Neural Networks RNN A family of neural architectures hidden states input sequence any length Core idea Apply the same weights ùëärepeatedly outputs optional 28A Simple RNN Language Model the students opened their words one hot vectors books laptops word embeddings a zoo output distribution Note this input sequence could be much longer now hidden states is the initial hidden state 28RNN Language Models the students opened their books laptops a zoo RNN Advantages Can process any length input Computation for step t can in theory use information from many steps back Model size doesn t increase for longer input context Same weights applied on every timestep so there is symmetry in how inputs are processed. RNN Disadvantages Recurrent computation is slow In practice difficult to access information from many steps back More on these later 30Training an RNN Language Model Get a big corpus of text which is a sequence of words Feed into RNN LM compute output distribution for every step t. i.e. predict probability dist of every word given words so far Loss function on step t is cross entropy between predicted probability distribution and the true next word one hot for Average this to get overall loss for entire training set 31Training an RNN Language Model negative log prob of students Loss Predicted prob dists Corpus the students opened their exams 32Training an RNN Language Model Loss Predicted prob dists negative log prob of opened Corpus the students opened their exams 33Training an RNN Language Model Loss Predicted prob dists negative log prob of their Corpus the students opened their exams 34Training an RNN Language Model Loss Predicted prob dists negative log prob of exams Corpus the students opened their exams 35Training an RNN Language Model Loss Predicted prob dists Teacher forcing Corpus the students opened their exams 36Training a RNN Language Model However Computing loss and gradients across entire corpus too expensive memory wise at once is In practice consider as a sentence or a document Recall Stochastic Gradient Descent allows us to compute loss and gradients for small chunk of data and update. Compute loss for a sentence actually a batch of sentences compute gradients and update weights. Repeat on a new batch of sentences. 37Backpropagation for RNNs Question What s the derivative of w.r.t. the repeated weight matrix Answer The gradient w.r.t. a repeated weight is the sum of the gradient w.r.t. each time it appears 38 WhyMultivariable Chain Rule Source https www.khanacademy.org math multivariable calculus multivariable derivatives differentiating vector valued functions a multivariable chain rule simple version 39Training the parameters of RNNs Backpropagation for RNNs 39 Question How do we calculate this Answer Backpropagate over timesteps i t 0 summing gradients as you go. This algorithm is called backpropagation through time Werbos P.G. 1988 Neural Networks 1 and others equals equals equals equals equals Apply the multivariable chain rule 1 In practice often truncated after 20 timesteps for training efficiency reasonsGenerating with an RNN Language Model Generating roll outs Just like an n gram Language Model you can use a RNN Language Model to generate text by repeated sampling. Sampled output becomes next step s input. s my favorite season my sample favorite sample season sample is sample is 40 spring sample spring s sampleGenerating text with an RNN Language Model Let s have some fun You can train an RNN LM on any kind of text then generate text in that style. RNN LM trained on Obama speeches Source https medium.com @samim obama rnn machine generated political speeches c8abd18a2ea0 42Generating text with an RNN Language Model Let s have some fun You can train an RNN LM on any kind of text then generate text in that style. RNN LM trained on Harry Potter Source https medium.com deep writing harry potter written by artificial intelligence 8a9431803da6 43Generating text with an RNN Language Model Let s have some fun You can train an RNN LM on any kind of text then generate text in that style. RNN LM trained on recipes Source https gist.github.com nylki 1efbaa36635956d35bcc 44Generating text with a RNN Language Model 44 Let s have some fun You can train a RNN LM on any kind of text then generate text in that style. RNN LM trained on paint color names Source http aiweirdness.com post 160776374467 new paint colors invented by neural network This is an example of a character level RNN LM predicts what character comes nextEvaluating Language Models The standard evaluation metric for Language Models is perplexity. This is equal to the exponential of the cross entropy loss Normalized by number of words Inverse probability of corpus according to Language Model Lower perplexity is better 46RNNs greatly improved perplexity over what came before n gram model Increasingly complex RNNs Perplexity improves lower is better Source https research.fb.com building an efficient neural language model over a billion words 474. Problems with RNNs Vanishing and Exploding Gradients 48Vanishing gradient intuition 49Vanishing gradient intuition chain rule 50Vanishing gradient intuition chain rule 51Vanishing gradient intuition chain rule 52Vanishing gradient intuition What happens if these are small Vanishing gradient problem When these are small the gradient signal gets smaller and smaller as it backpropagates further 53Vanishing gradient proof sketch linear case Recall What if were the identity function 53 Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf If Wh is small then this term gets exponentially problematic as becomes large chain rule Consider the gradient of the loss on step with respect to the hidden state on some previous step . Let chain rule value of ONLY READ IF INTERESTEDVanishing gradient proof sketch linear case What s wrong with Consider if the eigenvalues of are all less than 1 We can write as a basis What about nonlinear activations i.e. what we use Pretty much the same thing except the proof requires for some dependent on dimensionality and Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdf and supplemental materials at http proceedings.mlr.press v28 pascanu13 supp.pdf 54 eigenvectors using the eigenvectors of Approaches 0 as grows so gradient vanishes sufficient but not necessary ONLY READ IF INTERESTEDWhy is vanishing gradient a problem Gradient signal from far away is lost because it s much smaller than gradient signal from close by. So model weights are updated only with respect to near effects not long term effects. 56Effect of vanishing gradient on RNN LM 57 LM task When she tried to print her tickets she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer she finally printed her To learn from this training example the RNN LM needs to model the dependency between tickets on the 7th step and the target word tickets at the end. But if the gradient is small the model can t learn this dependency So the model is unable to predict similar long distance dependencies at test timeWhy is exploding gradient a problem If the gradient becomes too big then the SGD update step becomes too big learning rate gradient This can cause bad updates we take too large a step and reach a weird and bad parameter configuration with large loss You think you ve found a hill to climb but suddenly you re in Iowa In the worst case this will result in Inf or NaN in your network then you have to restart training from an earlier checkpoint 58Gradient clipping solution for exploding gradient 58 Gradient clipping if the norm of the gradient is greater than some threshold scale it down before applying SGD update Intuition take a step in the same direction but a smaller step In practice remembering to clip gradients is important but exploding gradients are an easy problem to solve Source On the difficulty of training recurrent neural networks Pascanu et al 2013. http proceedings.mlr.press v28 pascanu13.pdfHow to fix the vanishing gradient problem The main problem is that it s too difficult for the RNN to learn to preserve information over many timesteps. In a vanilla RNN the hidden state is constantly being rewritten First off next time How about an RNN with separate memory which is added to LSTMs And then Creating more direct and linear pass through connections in model Attention residual connections etc. 60Long Short Term Memory RNNs LSTMs A type of RNN proposed by Hochreiter and Schmidhuber in 1997 as a solution to the problem of vanishing gradients Everyone cites that paper but really a crucial part of the modern LSTM is from Gers et al. 2000 Only started to be recognized as promising through the work of S s student Alex Graves c. 2006 Work in which he also invented CTC connectionist temporal classification for speech recognition But only really became well known after Hinton brought it to Google in 2013 Following Graves having been a postdoc with Hinton 61 Hochreiter and Schmidhuber 1997. Long short term memory. https www.bioinf.jku.at publications older 2604.pdf Gers Schmidhuber and Cummins 2000. Learning to Forget Continual Prediction with LSTM. https dl.acm.org doi 10.1162 089976600300015015 Graves Fernandez Gomez and Schmidhuber 2006. Connectionist temporal classification Labelling unsegmented sequence data with recurrent neural nets. https www.cs.toronto.edu graves icml_2006.pdfLong Short Term Memory RNNs LSTMs 62 On step t there is a hidden state ùíâ ùë° and a cell state ùíÑ ùë° Both are vectors length n The cell stores long term information The LSTM can read erase and write information from the cell The cell becomes conceptually rather like RAM in a computer The selection of which information is erased written read is controlled by three corresponding gates The gates are also vectors of length n On each timestep each element of the gates can be open 1 closed 0 or somewhere in between The gates are dynamic their value is computed based on the current contextWe have a sequence of inputs ùë• ùë° and we will compute a sequence of hidden states ‚Ñé ùë° and cell states ùëê ùë° . On timestep t Long Short Term Memory LSTM All these are vectors of same length n Forget gate controls what is kept vs forgotten from previous cell state Input gate controls what parts of the new cell content are written to cell Output gate controls what parts of cell are output to hidden state New cell content this is the new content to be written to the cell Cell state erase forget some content from last cell state and write input some new cell content Hidden state read output some content from the cell Sigmoid function all gate values are between 0 and 1 20 Gates are applied using element wise or Hadamard productLong Short Term Memory LSTM You can think of the LSTM equations visually like this Source http colah.github.io posts 2015 08 Understanding LSTMs 64ct 1 ht 1 ct ht ft it ot ct t c Long Short Term Memory LSTM You can think of the LSTM equations visually like this Compute the forget gate Forget some cell content Compute the input gate Compute the new cell content Compute the output gate Write some new cell content Output some cell content to the hidden state The sign is the secret Source http colah.github.io posts 2015 08 Understanding LSTMs 65How does LSTM solve vanishing gradients 66 The LSTM architecture makes it much easier for an RNN to preserve information over many timesteps e.g. if the forget gate is set to 1 for a cell dimension and the input gate set to 0 then the information of that cell is preserved indefinitely. In contrast it s harder for a vanilla RNN to learn a recurrent weight matrix Wh that preserves info in the hidden state In practice you get about 100 timesteps rather than about 7 However there are alternative ways of creating more direct and linear pass through connections in models for long distance dependenciesIs vanishing exploding gradient just an RNN problem For example Residual connections aka ResNet Also known as skip connections The identity connection preserves information by default This makes deep networks much easier to train 24 No It can be a problem for all neural architectures including feed forward and convolutional especially very deep ones. Due to chain rule choice of nonlinearity function gradient can become vanishingly small as it backpropagates Thus lower layers are learned very slowly i.e. are hard to train Another solution lots of new deep feedforward convolutional architectures add more direct connections thus allowing the gradient to flow Deep Residual Learning for Image Recognition He et al 2015. https arxiv.org pdf 1512.03385.pdfLSTMs real world success 68 In 2013 2015 LSTMs started achieving state of the art results Successful tasks include handwriting recognition speech recognition machine translation parsing and image captioning as well as language models LSTMs became the dominant approach for most NLP tasks Now 2019 2023 Transformers have become dominant for all tasks For example in WMT a Machine Translation conference competition In WMT 2014 there were 0 neural machine translation systems In WMT 2016 the summary report contains RNN 44 times and these systems won In WMT 2019 RNN 7 times Transformer 105 times Source Findings of the 2016 Conference on Machine Translation WMT16 Bojar et al. 2016 http www.statmt.org wmt16 pdf W16 2301.pdf Source Findings of the 2018 Conference on Machine Translation WMT18 Bojar et al. 2018 http www.statmt.org wmt18 pdf WMT028.pdf Source Findings of the 2019 Conference on Machine Translation WMT19 Barrault et al. 2019 http www.statmt.org wmt18 pdf WMT028.pdf5. Recap 69 Language Model A system that predicts the next word Recurrent Neural Network A family of neural networks that Take sequential input of any length Apply the same weights on each step Can optionally produce output on each step Recurrent Neural Network Language Model We ve shown that RNNs are a great way to build a LM despite some problems RNNs are also useful for much moreWhy should we care about Language Modeling 70 Language Modeling is a benchmark task that helps us measure our progress on predicting language use Language Modeling is a subcomponent of many NLP tasks especially those involving generating text or estimating the probability of text Predictive typing Speech recognition Handwriting recognition Spelling grammar correction Authorship identification Machine translation Summarization Dialogue etc. Everything else in NLP has now been rebuilt upon Language Modeling GPT 3 is an LMOther RNN ArchitecturesOther RNN uses RNNs can be used for sequence tagging e.g. part of speech tagging named entity recognition knocked over the vase the startled cat 72 VBN IN DT NN DT JJ NNthe movie a lot overall I enjoyed positive Sentence encoding 73 How to compute sentence encoding RNNs can be used for sentence classification e.g. sentiment classificationthe movie a lot overall I enjoyed positive Sentence encoding 74 equals How to compute sentence encoding Basic way Use final hidden state RNNs can be used for sentence classification e.g. sentiment classificationRNNs can be used for sentence classification the movie a lot overall I enjoyed positive Sentence encoding How to compute sentence encoding 75 Usually better Take element wise max or mean of all hidden states e.g. sentiment classificationRNNs can be used as an encoder module e.g. question answering machine translation many other tasks Context Ludwig van Beethoven was a German composer and pianist. A crucial figure Beethoven Question what nationality was Here the RNN acts as an encoder for the Question the hidden states represent the Question . The encoder is part of a larger neural system. Answer German 76 lots of neural architecture lots of neural architectureRNN LMs can be used to generate text e.g. speech recognition machine translation summarization RNN LM what s the the weather what s This is an example of a conditional language model. We ll see Machine Translation in much more detail next lectures. Input audio START conditioning 77Bidirectional RNNs was terribly exciting the movie Forward RNN Backward RNN Concatenated hidden states This contextual representation of terribly has both left and right context 78Bidirectional RNNs 79 Note bidirectional RNNs are only applicable if you have access to the entire input sequence They are not applicable to Language Modeling because in LM you only have left context available. If you do have entire input sequence e.g. any kind of encoding bidirectionality is powerful you should use it by default . For example BERT Bidirectional Encoder Representations from Transformers is a powerful pretrained contextual representation system built on bidirectionality. You will learn more about transformers including BERT in a couple of weeksMulti layer RNNs RNNs are already deep on one dimension they unroll over many timesteps We can also make them deep in another dimension by applying multiple RNNs this is a multi layer RNN. This allows the network to compute more complex representations The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. Multi layer RNNs are also called stacked RNNs. 80Multi layer RNNs was terribly exciting the movie RNN layer 1 RNN layer 2 81 RNN layer 3 The hidden states from RNN layer i are the inputs to RNN layer i 1Multi layer RNNs in practice 39 Multi layer or stacked RNNs allow a network to compute more complex representations they work better than just have one layer of high dimensional encodings The lower RNNs should compute lower level features and the higher RNNs should compute higher level features. High performing RNNs are usually multi layer but aren t as deep as convolutional or feed forward networks For example In a 2017 paper Britz et al. find that for Neural Machine Translation 2 to 4 layers is best for the encoder RNN and 4 layers is best for the decoder RNN Often 2 layers is a lot better than 1 and 3 might be a little better than 2 Usually skip connections dense connections are needed to train deeper RNNs e.g. 8 layers Transformer based networks e.g. BERT are usually deeper like 12 or 24 layers. You will learn about Transformers later they have a lot of skipping like connections Massive Exploration of Neural Machine Translation Architecutres Britz et al 2017. https arxiv.org pdf 1703.03906.pdfTerminology By the end of the course You will understand phrases like stacked bidirectional LSTMs with residual connections and self attention The RNN described in this lecture simple vanilla Elman RNN You learned about other RNN flavors like LSTM and GRU and multi layer RNNs 83